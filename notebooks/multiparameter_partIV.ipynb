{
 "metadata": {
  "name": "multiparameter_partIV"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "General properties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "By this point, we've seen many examples of exponential families as well as some algorithms related to them:\n",
      "\n",
      "1. maximum likelihood estimation $\\CGF^*$ via Newton-Raphson, constrained to remain in $\\D(\\CGF)$;\n",
      "\n",
      "2. constrained MLE: the Lagrange multiplier method;\n",
      "    \n",
      "3. stochastic approximation: when the $\\CGF$ is too complicated to compute;\n",
      "    \n",
      "4. Gibbs sampler: to simulate from $\\Pp_{\\eta}$;\n",
      "    \n",
      "5. pseudo-likelihood.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We now remind ourselves of a few general properties before considering the most \n",
      "*popular* class of exponential families: *generalized linear models*.\n",
      "   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Likelihoods, scores, and Fisher information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The definitions introduced for one-parameter families are readily generalized to the multiparameter situation. Indeed, we have already been using some of them. Below,\n",
      "we assume that we have sampled $Y_i \\overset{IID}{\\sim} \\Pp_{\\eta}, 1 \\leq i \\leq n.$\n",
      "\n",
      "1. The *log-likelihood* is\n",
      "$$\n",
      "\\ell(\\eta;\\yy) = \\ell(\\eta)  =  n[\\etabold^T\\bar{\\yy}\\\n",
      " -\\CGF(\\etabold)].\n",
      "$$\n",
      "\n",
      "2. The  *score function for $\\etabold$* is\n",
      "$$\n",
      "%\\label{eqn:score-etavector}\n",
      "\\nabla \\ell(\\eta)      =  n(\\bar{\\yy} - \\mubold).\n",
      "$$\n",
      "\n",
      "3. The *Fisher information for $\\etabold$*, denoted $\\ii_{\\etabold}^{(n)}$, is\n",
      "$$\n",
      "\\ii_{\\etabold}^{(n)}  \\equiv  \\EE_{\\etabold}\\left( \\nabla \\ell_{\\eta} \\nabla \\ell_{\\eta}^T\\right)  =  n\\VV_{\\eta}   = \\VV^{(n)}_{\\eta}\n",
      " =  -\\nabla^2 \\ell_{\\etabold}.\n",
      "$$\n",
      "\n",
      "4. The *score function for $\\mubold$* is\n",
      "$$\n",
      "\\frac{\\partial}{\\partial\\mubold}\\ell_{\\etabold}(\\yy) \\,\\, = \\,\\, \\left(\\frac{\\VV_{\\eta}}{n}\\right)^{-1}(\\bar{\\yy}-\\mubold).\n",
      "$$\n",
      "\n",
      "\n",
      "\n",
      "5. The *Fisher information for $\\mubold$*, denoted $\\ii_{\\etabold}^{(n)}(\\mubold)$, is\n",
      "$$\n",
      "\\ii_{\\etabold}^{(n)}(\\mubold)  =  \\VV^{-1}_{\\eta}i_{\\etabold}^{(n)}\\VV^{-1}_{\\eta}  =  n\\VV^{-1}_{\\eta}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "By differentiating the score function for $\\eta$, we see that $\\ell_{\\etabold}(\\yy)$ is a concave function of $\\etabold$, since $\\ddot{\\ell}_{\\etabold}=-n\\VV_{\\etabold} \\leq 0$.  In other words, the density \n",
      "$$\n",
      "\\frac{d\\Pp_{\\eta}}{dm}\n",
      "$$\n",
      "is *log-concave*.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Cramer-Rao lower bound"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For real-valued $\\zeta=h(\\etabold)$, the *Cramer-Rao lower bound* for any unbiased estimator $\\bar{\\zeta}$ is\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\Vv_{\\eta}(\\bar{\\zeta})& \\geq \\frac{\\nabla{h}^T_{\\eta} \\VV^{-1}_{\\eta}\\nabla{h}_{\\eta}}{n}  \\\\\n",
      "& = \\nabla{h}_{\\eta}^T(\\ii_{\\etabold}^{(n)})^{-1}\\nabla{h}_{\\eta},\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "For $\\zeta=H(\\mubold)$, the bound takes the form is\n",
      "$$\n",
      "\\Vv_{\\eta}(\\bar{\\zeta}) \\geq \\nabla H_{\\mu}^T[{\\ii_{\\etabold}^{(n)}(\\mubold)}]^{-1}\n",
      " \\nabla H_{\\mu}.\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Notice that $ \\nabla H_{\\mu} ^T[{\\ii_{\\etabold}^{(n)}(\\mubold)}]^{-1} \\nabla H_{\\mu} $ is the delta method estimate of variance for $\\hat{\\zeta}=H(\\hat{\\mubold})=H(\\bar{\\yy})$.  In practice the variance estimate often used is\n",
      "$$\n",
      "\\widehat{\\Vv}_{\\etabold}(\\hat{\\zeta}) = \n",
      "\\nabla h_{\\hat{\\eta}}^T (\\ii_{\\hat{\\eta}}^{(n)})^{-1}\\nabla{h}_{\\hat{\\eta}}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For the $\\mubold$ itself, $\\hat{\\mubold} = \\bar{\\yy}$ is unbiased. We also see that\n",
      "it achieves the CRLB:\n",
      "\\begin{eqnarray}\n",
      "\\Vv_{\\etabold}(\\hat{\\mubold}) \\overset{\\rm CRLB}{=} \\VV_{\\eta}/n  =  {i_{\\etabold}^{(n)}(\\mubold)}^{-1}\n",
      "\\end{eqnarray}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Distribution of the MLE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The CLT based on repeated sampling asserts that\n",
      "$$\n",
      "\\hat{\\mu} \\approx N(\\mu, \\VV_{\\eta}/n).\n",
      "$$\n",
      "While the mean and variance are correct, the approximation above is in the distribution,\n",
      "i.e. the distributional sense.\n",
      "\n",
      "Applying the delta rule yields\n",
      "$$\n",
      "\\hat{\\eta} \\approx N(\\eta, \\VV_{\\eta}^{-1}/n).\n",
      "$$\n",
      "In this case, the mean and variance are not exact, nor is the distribution exactly normal."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Skewness and kurtosis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For multiparameter families, the mean is a $p$-vector, the covariance a $p \\times p$ matrix. These are, of course, just derivatives of $\\CGF$. Higher order derivatives\n",
      "are *tensors*. \n",
      "\n",
      "For skewness, there is a $p \\times p \\times p$ tensor, or multidimensional array\n",
      "that takes three vectors as argument. That is,\n",
      "$$\n",
      "\\nabla^3 \\CGF_{\\eta}(U,V,W) \\in \\real.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Formally, the power series for $\\CGF$ can be written as\n",
      "$$\n",
      "\\CGF(\\eta + \\Delta) = \\sum_{j=0}^{\\infty} \\frac{1}{j!} \\nabla^j \\CGF_{\\eta}(\\otimes^j \\Delta).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: skewness of a one-parameter subfamily*\n",
      "\n",
      "Suppose we consider the one-parameter subfamily\n",
      "$$\n",
      "\\left\\{\\eta=\\eta_0 + s \\cdot v, s \\in \\real, \\eta \\in \\D(\\CGF)\\right\\}.\n",
      "$$\n",
      "\n",
      "1. What is the sufficient statistic of this one-parameter subfamily (denote the \n",
      "sufficient statistic of the original family by $t(x)$).   \n",
      "\n",
      "2. Express the skewness and kurtosis of this sufficient statistic in terms of the\n",
      "derivatives of $\\CGF$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stein's least favorable family"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Suppose we want to estimate $\\zeta=h(\\eta)=h(\\nabla \\CGF^*(\\mu))=s(\\mu)$, a real-valued\n",
      "function of $\\eta$.\n",
      "\n",
      "Stein's least favorable family is a one-parameter subfamily of the original family, determined by some hypothesized true value, say, $\\eta_0$ and the function $h$ (or, equivalently, $s$).\n",
      "\n",
      "The family is\n",
      "$$\n",
      "\\left\\{\\eta: \\eta=\\eta_0 + \\theta \\cdot \\VV_{\\eta_0}^{-1} \\nabla h_{\\eta_0}, \n",
      "\\theta \\in \\real, \\eta \\in \\D(\\CGF) \\right\\}.\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: Stein's least favorable family*\n",
      "\n",
      "1. Show that the CRLB for the above one-parameter family is the same as the\n",
      "CRLB in the full $p$-parameter family.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Deviance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Everything we saw previously about deviance remains true in the\n",
      "multiparameter case.\n",
      "\n",
      "$$\n",
      "\\begin{aligned}\n",
      "D(\\eta_1;\\eta_2) &= D(\\Pp_{\\eta_1};\\Pp_{\\eta_2})  \\\\\n",
      "&= 2 \\left[\\CGF(\\eta_2)-\\CGF(\\eta_1) - \\nabla \\CGF(\\eta_1)^T(\\eta_2-\\eta_1)\\right] \\\\\n",
      "&= 2 \\left[\\CGF^*(\\mu_1)-\\CGF^*(\\mu_2) - \\nabla \\CGF^*(\\mu_2)^T(\\mu_1-\\mu_2) \\right] \\\\\n",
      "&= \\tilde{D}(\\mu_1;\\mu_2).\n",
      "\\end{aligned}\n",
      "$$\n",
      "with $\\mu_i=\\nabla \\CGF(\\eta_i)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The deviance and Fisher information are both related to \n",
      "Taylor series expansions of $\\CGF$.\n",
      "\n",
      "$$\n",
      "\\begin{aligned}\n",
      "D(\\eta_1;\\eta_2) &= \\nabla^2 \\CGF_{\\eta_1}(\\eta_2-\\eta_1,\\eta_2-\\eta_1) + O (\\|\\eta_2-\\eta_1\\|^3) \\\\\n",
      "&= (\\eta_2-\\eta_1)^T \\ii_{\\etabold}^{(n)}(\\eta_2-\\eta_1) + O (\\|\\eta_2-\\eta_1\\|^3).\n",
      "\\end{aligned}\n",
      "$$\n",
      "where the constant in the remainder can be expressed in terms of a Lipschitz constant of\n",
      "$\\nabla^3 \\CGF$ near $\\eta_1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: monotone mapping*\n",
      "\n",
      "Show that, in general\n",
      "$$\n",
      "(\\eta_2-\\eta_1)^T(\\mu_2-\\mu_1) = \\frac{1}{2} \\left[D(\\eta_1;\\eta_2)+D(\\eta_2;\\eta_1)\\right] \\geq 0\n",
      "$$\n",
      "with $\\mu_i = \\nabla \\CGF_{\\eta_i}$.\n",
      "\n",
      "This demonstrates that the relationship between is globally monotone, and is an example\n",
      "of the general concept of a [monotone mapping](http://www.stanford.edu/class/ee364b/lectures/monotone_slides.pdf), the canonical example being the (sub)gradient of a convex function."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Generalized linear models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A generalized linear model (GLM) is a model the conditional\n",
      "distribution \n",
      "$$Y|X, X \\in \\real^p, Y \\in \\real.$$\n",
      "\n",
      "The distribution of $Y|X$ is assumed to be in some\n",
      "one parameter exponential family $\\Pp_{\\eta}$ with sufficient statistic $Y$.\n",
      "\n",
      "Therefore,\n",
      "$$\n",
      "Y|X \\sim \\Pp_{\\eta(X)}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The *linear* in GLM refers to the assumption that\n",
      "$$\n",
      "\\eta(x) = x^T\\beta, \\beta \\in \\real^p.\n",
      "$$\n",
      "\n",
      "The *model* refers to the assumption that we observe\n",
      "$$\n",
      "Y_i | X_i \\overset{\\text{indep}} {\\sim} \\Pp_{\\eta(x_i)}, \\qquad 1 \\leq i \\leq n,\n",
      "$$\n",
      "where the independence here is a statement on the distribution of the vector $Y$ given the\n",
      "matrix \n",
      "$$\n",
      "X_{n \\times p} = \\begin{pmatrix}\n",
      "x_1^T \\\\\n",
      "\\vdots \\\\\n",
      "x_n^T\n",
      "\\end{pmatrix}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The parameters of the GLM are $\\beta$, and the loglikelihood is\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\ell(\\beta) &= \\sum_{i=1}^n \\eta_i(\\beta) \\cdot y_i - \\CGF(\\eta_i(\\beta)) \\\\\n",
      "&= \\sum_{i=1}^n (x_i^T\\beta) \\cdot y_i - \\CGF(x_i^T\\beta). \\\\\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We could write this in matrix form as\n",
      "$$\n",
      "\\ell(\\beta) = (X\\beta)^TY - \\CGF^{(n)}(X\\beta) = \\beta^T(X^TY) - \\CGF^{(n)}(X\\beta)\n",
      "$$\n",
      "where\n",
      "$$\n",
      "\\CGF^{(n)}(\\eta) = \\sum_{i=1}^n \\CGF(\\eta_i), \\eta \\in \\real^n.\n",
      "$$\n",
      "\n",
      "Written in this way, we see that the GLM is actually a $p$-parameter \n",
      "exponential family with sufficient statistic $X^TY$ and CGF $\\tilde{\\CGF}(\\beta) = \\CGF^{(n)}(X\\beta)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's untangle this statement a little more carefully. \n",
      "\n",
      "1. If it \n",
      "is an exponential family, it should be a collection of distributions. In this\n",
      "case it is a collection of conditional distributions for $Y|X$. These are\n",
      "distributions on $\\real^n$.\n",
      "\n",
      "2. If it is an exponential family, it should have a reference measure.\n",
      "In this case, the reference measure is the product\n",
      "$$\n",
      "\\prod_{i=1}^n m(dy_i)\n",
      "$$\n",
      "where $m$ is the reference measure of $\\Pp_{\\eta}$.\n",
      "\n",
      "3. If it is an exponential family, it should have a natural parameter and\n",
      "a sufficient statistic. In this case, the natural parameter is $\\beta$ and the\n",
      "sufficient statistic is $X^TY$.\n",
      "\n",
      "4. The fact that the $\\tilde{\\CGF}(\\beta)=\\CGF^{(n)}(X\\beta)$ follows directly from the likelihood and the modelling assumption that\n",
      "$\\eta_i=x_i^T\\beta$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Strictly speaking, this GLM is one in which we have used the *canonical link*, i.e.\n",
      "$$\n",
      "\\eta_i = x_i^T\\beta.\n",
      "$$\n",
      "\n",
      "A model with a non-canonical link has\n",
      "$$\n",
      "\\eta_i = F(x_i^T\\beta).\n",
      "$$\n",
      "We will see examples shortly."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: link functions*\n",
      "\n",
      "Suppose we use a non-canonical link function. Is the collection of distributions\n",
      "$Y|X$ an exponential family with natural parameter $\\beta$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: GLM computations*\n",
      "\n",
      "1. Show that \n",
      "$$\n",
      "\\Ee_{\\beta}(X^TY) = X^T \\nabla \\CGF^{(n)}_{X\\beta}.\n",
      "$$\n",
      "\n",
      "2. Show that\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\Vv_{\\beta}(X^TY) &= X^T \\nabla^2 \\CGF^{(n)}_{X\\beta} X \\\\\n",
      "&= X^T \\text{diag}(\\ddot \\CGF(x_i^T\\beta)) X.\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Score equation, Fisher information for GLM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The score equation for $\\beta$ is \n",
      "$$\n",
      "\\nabla \\ell_{\\beta} = X^T(Y-\\nabla \\CGF^{(n)}_{X\\beta}) = X^T(Y-\\mu(\\beta))\n",
      "$$\n",
      "where\n",
      "$$\n",
      "\\mu(\\beta) = \\nabla \\CGF^{(n)}_{X\\beta} \\in \\real^n.\n",
      "$$\n",
      "\n",
      "The Fisher information for $\\beta$ is \n",
      "$$\n",
      "- \\nabla^2 \\ell_{\\beta} = X^T \\text{diag}(\\ddot \\CGF(x_i^T\\beta)) X.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This diagonal matrix can be thought of as assigning a weight to each case, so we define\n",
      "$$\n",
      "W_{\\beta} = \\text{diag}(\\ddot \\CGF(x_i^T\\beta))\n",
      "$$\n",
      "and the Fisher information has the form\n",
      "$$\n",
      "-\\nabla^2 \\ell_{\\beta} = \\ii^{(n)}_{\\beta} =  X^TW_{\\beta}X.\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Asymptotic distribution of MLE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The general asymptotic picture for exponential families yields\n",
      "$$\n",
      "\\hat{\\beta} \\approx N(\\beta, (\\ii^{(n)}_{\\beta})^{-1}) = N(\\beta, (X^TW_{\\beta}X)^{-1}).\n",
      "$$\n",
      "Above, both the mean and variance are approximate, as is the distribution itself.\n",
      "\n",
      "After estimation of the MLE, this distribution is usually approximated as\n",
      "$$\n",
      "N(\\beta, X^TW_{\\hat{\\beta}}X).\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Most generalized linear models are such that $\\tilde{\\CGF}(\\beta)=\\real^p$ so unconstrained Newton-Raphson can be used.\n",
      "\n",
      "The algorithm looks like\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\hat{\\beta}^{(k+1)} &= \\hat{\\beta}^{(k)} - \\nabla^2 \\tilde{\\CGF}\\left(\\hat{\\beta}^{(k)}\\right)^{-1} \\left[ \\nabla \\tilde{\\CGF}\\left(\\hat{\\beta}^{(k)} \\right) - X^TY \\right] \\\\\n",
      "&= \\hat{\\beta}^{(k)} - (X^TW_{\\hat{\\beta}^{(k)}}X)^{-1} \\left[X^T \\left(\\mu(\\hat{\\beta}^{(k)})-Y \\right) \\right]\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MLE map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "So far, for all of our exponential families, we have written the MLE map \n",
      "in terms of the Fenchel-Legendre transform of a CGF. The GLM should be no different.\n",
      "\n",
      "The mean value space is $\\nabla \\tilde{\\CGF}(\\real^p) \\subset \\real^p$ (I am assuming the\n",
      "$\\D(\\CGF)=\\real$ here), and the MLE map should be a map from\n",
      "$\\Mm$, the convex hull of $\\nabla \\tilde{\\CGF}(\\real^p)$ back to $\\real^p$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "To find the Fenchel-Legendre transform, we would have to find\n",
      "$$\n",
      "\\tilde{\\CGF}^*(\\xi) = \\sup_{\\beta} \\xi^T\\beta - \\tilde{\\CGF}(\\beta).\n",
      "$$\n",
      "Or, we would solve\n",
      "$$\n",
      "\\minimize_{\\beta} \\tilde{\\CGF}(\\beta) - \\xi^T\\beta.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In the GLM case, it is somewhat easier to use a dual function rather than\n",
      "the $\\tilde{\\CGF}^*$ itself, because we have an implicit constraint, namely\n",
      "$$\n",
      "\\eta_i=x_i^T\\beta.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As we did in the one parameter case, let's form the Lagrangian\n",
      "$$\n",
      "\\begin{aligned}\n",
      "L(\\eta,\\beta;r) &= (X\\beta)^TY - \\tilde{\\CGF}(\\beta) + r^T(X\\beta-\\eta) \\\\\n",
      "&= \\eta^TY - \\tilde{\\CGF}(\\beta) + r^T(X\\beta-\\eta) \\\\\n",
      "&= \\sum_{i=1}^n\\left[\\eta_i^Ty_i - \\CGF(\\eta_i) + r_i(x_i^T\\beta-\\eta_i)\\right] \\\\\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "Above, we have used \n",
      "$$\n",
      "\\tilde{\\CGF}(\\beta) = \\sum_{i=1}^n \\CGF(x_i^T\\beta).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Minimizing over $\\eta$ yields\n",
      "$$\n",
      "-\\sum_{i=1}^n \\CGF^*(y_i-r_i)\n",
      "$$\n",
      "while minimizing over $\\beta$ yields a constraint \n",
      "$$\n",
      "\\sum_{i=1}^n r_ix_i^T = 0.\n",
      "$$\n",
      "Or, $r \\in \\text{null}(X^T)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Dual problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "This is the general dual problem\n",
      "$$\n",
      "\\minimize_{r: X^Tr=0} \\sum_{i=1}^n \\CGF^*(y_i-r_i).\n",
      "$$\n",
      "\n",
      "Having found $\\hat{r}$, the same calculation we saw for the Lagrange multipliers\n",
      "in the one-parameter case tell us that\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\hat{\\eta}_i &= \\dot\\CGF^*(y_i-\\hat{r}_i) \\\\\n",
      "&= X\\hat{\\beta}_i.\n",
      "\\end{aligned}\n",
      "$$\n",
      "In vector form, this reads as\n",
      "$$\n",
      "\\hat{\\eta} = \\nabla \\left(\\CGF^{(n)}\\right)^*(Y - \\hat{r}).\n",
      "$$\n",
      "\n",
      "This yields\n",
      "$$\n",
      "\\hat{\\beta} = X^{\\dagger}\\left[\\left(\\nabla{\\CGF}^{(n)} \\right)^*(Y-\\hat{r}) \\right].\n",
      "$$\n",
      "where $X^{\\dagger}$ is the pseudoinverse with\n",
      "$$\n",
      "X^{\\dagger} = (X^TX)^{-1}X^T\n",
      "$$\n",
      "when $X^TX$ is non-singular."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### *Exercise: variables in the dual problem*\n",
      "\n",
      "The notation $r$ for the variables in the dual problem suggest *residual*.\n",
      "\n",
      "1. Show that \n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\hat{r} &= Y - \\nabla \\CGF^{(n)}(X\\hat{\\beta}) \\\\\n",
      "&= Y - \\hat{\\mu}.\n",
      "\\end{aligned}\n",
      "$$\n",
      "                                "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Projection picture"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\n",
      "The GLM is a $p$-parameter subfamily of the $n$-parameter family\n",
      "$$\n",
      "\\prod_{i=1}^n \\Pp_{\\eta_i}.\n",
      "$$\n",
      "\n",
      "When $p > n$, it can even be the same family in the sense that\n",
      "the map\n",
      "$$\n",
      "\\beta \\mapsto X\\beta\n",
      "$$\n",
      "covers all of $\\real^n$.\n",
      "\n",
      "In this case, the MLE is not generally unique. For $p \\leq n$, the set\n",
      "$$\n",
      "\\Mm_X = \\left\\{\\nabla \\CGF^n(X \\beta), \\beta \\in \\real^p\\right\\} = \\left\\{\\mu(\\beta): \\beta \\in \\real^p \\right\\}\n",
      "$$\n",
      "describes a curved subset of $\\real^n$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The score equation for $\\beta$ is\n",
      "$$\n",
      "X^T(Y - \\mu(\\hat{\\beta}))=0.\n",
      "$$\n",
      "Brad expresses this by saying that the MLE projects $Y$ onto $\\Mm_X$ \"orthogonal\" to the columns of $X$.\n",
      "\n",
      "This same score equation almost arises when we consider the least squares\n",
      "projection of $Y$ onto $\\Mm_X$. That is, if we try to solve the following problem\n",
      "$$\n",
      "\\minimize_{\\mu \\in \\Mm_X} \\|Y-\\mu\\|^2_2.\n",
      "$$\n",
      "\n",
      "Then, we see that critical points in $\\Mm_X$, when parameterizing\n",
      "the problem by $\\beta$ satisfy\n",
      "$$\n",
      "Y-\\mu(\\hat{\\beta}) \\perp T_{\\mu(\\hat{\\beta})}\\Mm_X\n",
      "$$\n",
      "where the last expression is the tangent space to the curved surface $\\Mm_X$ at\n",
      "$\\mu=\\mu(\\hat{\\beta})$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: tangent space computation*\n",
      "\n",
      "1. Show that the tangent space at $\\mu(\\hat{\\beta})$ is\n",
      "made up of the span of the vectors\n",
      "$$\n",
      "\\left(x_{1j} \\ddot{\\CGF}(x_1^T\\beta), \\dots,  x_{nj} \\ddot{\\CGF}(x_n^T\\beta)\\right), 1 \\leq j \\leq p.\n",
      "$$\n",
      "\n",
      "2. Write this in matrix form.\n",
      "\n",
      "3. Use this to fully write out the stationary condition for \n",
      "$$\n",
      "\\minimize_{\\mu \\in \\Mm_X} \\|Y-\\mu\\|^2_2.\n",
      "$$\n",
      "Is this the same as the MLE stationarity equation?\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example: using Poisson regression for density estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "In this example, we will use Poisson regression to fit a density to the\n",
      "$t$-statistics we saw in our application of Tweedie's formula.\n",
      "Recall, we had $N=6033$ $t$-statistics for comparing a healthy \n",
      "population and to a cancerous one.\n",
      "\n",
      "Our model will be that the density has the form\n",
      "$$\n",
      "f_{\\beta}(x) = \\exp \\left(\\sum_{j=0}^7 \\beta_j x^j - \\CGF(\\beta)\\right).\n",
      "$$\n",
      "This is an 8-parameter exponential family of distributions on $\\real$ with\n",
      "Lebesgue as reference measure."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The basic approach will be to use the Poisson trick we saw in the conditioning\n",
      "section. \n",
      "\n",
      "1. We discretize $\\real$ into $K=50$ equal\n",
      "sized bins of size $\\Delta$ $B_i=(L_i,U_i], 1 \\leq i \\leq K$ and\n",
      "define $$Y_i = \\# \\left\\{Z_j: Z_j \\in B_i\\right\\}$$.\n",
      "                                                                \n",
      "2. We model $Y_i \\sim \\text{Poisson}(\\mu_i)$ where\n",
      "$$\n",
      "\\mu_i = N \\cdot \\int_{(L_i,U_i]} f_{\\beta}(x) \\;dx \\approx N \\Delta \\sum_{j=0}^7 \\beta_j M_i^j \n",
      "$$                                                                \n",
      "where $N$ is the number of observed $Z$'s and $M_i=(L_i+U_i)/2$ is the midpoint of the interval.\n",
      "\n",
      "3. We fit a Poisson GLM to the $Y_i$ with design matrix\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "1 & M_1 & M_1^2 & \\dots & M_1^7 \\\\\n",
      "1 & M_2 & M_2^2 & \\dots & M_2^7 \\\\\n",
      "\\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
      "1 & M_K & M_K^2 & \\dots & M_K^7 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "Actually, this design matrix is highly singular. We use `R`'s `poly` function which\n",
      "produces a design matrix with the same column span but is much better conditioned.\n",
      "\n",
      "4. This yields an estimated density\n",
      "$$\n",
      "\\hat{f}_{\\beta}(x) \\propto \\exp \\left(\\sum_{j=0}^7 \\hat{\\beta}_j x^j \\right).\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(sda)\n",
      "data(singh2002)\n",
      "labels = singh2002$y\n",
      "print(summary(labels))\n",
      "expression_data = singh2002$x\n",
      "tvals = c()\n",
      "for (i in 1:6033) {\n",
      "    tvals = c(tvals, t.test(expression_data[,i] ~ labels, var.equal=TRUE)$statistic)\n",
      "}\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "Loading required package: entropy\n",
        "Loading required package: corpcor\n",
        "Loading required package: fdrtool\n",
        " cancer healthy \n",
        "     52      50 \n",
        "Warning messages:\n",
        "1: package \u2018sda\u2019 was built under R version 2.15.3 \n",
        "2: package \u2018entropy\u2019 was built under R version 2.15.3 \n",
        "3: package \u2018corpcor\u2019 was built under R version 2.15.3 \n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "As in the previous exercise, we will convert to $Z$-scores, though this has hardly\n",
      "any effect."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R -o zvals\n",
      "zvals = qnorm(pt(tvals, 100))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = plt.hist(zvals,bins=50)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD9CAYAAABDaefJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1087b3e50>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Next, let's split the data into bins, and compute the counts and the midpoints."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "bins = c(-Inf, seq(-4,4,length=51), Inf)\n",
      "counts = c()\n",
      "for (i in 1:(length(bins)-1)) {\n",
      "    counts = c(counts, sum((zvals > bins[i]) * (zvals <= bins[i+1])))\n",
      "}\n",
      "midpoints = (bins[1:length(bins)-1] + bins[2:length(bins)])/2\n",
      "counts = counts[2:(length(counts)-1)]\n",
      "midpoints = midpoints[2:(length(midpoints)-1)]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "To fit a GLM in `R`, we use the `glm` function. The argument \n",
      "`family` says that we are using a Poisson regression model, which sets the\n",
      "family $\\Pp_{\\eta}$ above to be a Poisson family."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "density.glm = glm(counts ~ poly(midpoints, 7), family=poisson(link='log'))\n",
      "print(summary(density.glm))\n",
      "X = model.matrix(density.glm)\n",
      "C = density.glm$coef\n",
      "SE = summary(density.glm)$coef[,2]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "\n",
        "Call:\n",
        "glm(formula = counts ~ poly(midpoints, 7), family = poisson(link = \"log\"))\n",
        "\n",
        "Deviance Residuals: \n",
        "     Min        1Q    Median        3Q       Max  \n",
        "-1.79431  -0.69422   0.01274   0.61035   2.08374  \n",
        "\n",
        "Coefficients:\n",
        "                     Estimate Std. Error z value Pr(>|z|)    \n",
        "(Intercept)           3.89067    0.03436 113.241  < 2e-16 ***\n",
        "poly(midpoints, 7)1  -0.19805    0.35327  -0.561    0.575    \n",
        "poly(midpoints, 7)2 -10.89546    0.35131 -31.014  < 2e-16 ***\n",
        "poly(midpoints, 7)3  -0.14374    0.32855  -0.437    0.662    \n",
        "poly(midpoints, 7)4   1.99022    0.30931   6.434 1.24e-10 ***\n",
        "poly(midpoints, 7)5   0.01894    0.30917   0.061    0.951    \n",
        "poly(midpoints, 7)6   0.31586    0.20410   1.548    0.122    \n",
        "poly(midpoints, 7)7   0.07490    0.20382   0.367    0.713    \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n",
        "\n",
        "(Dispersion parameter for poisson family taken to be 1)\n",
        "\n",
        "    Null deviance: 6707.369  on 49  degrees of freedom\n",
        "Residual deviance:   41.079  on 42  degrees of freedom\n",
        "AIC: 342.61\n",
        "\n",
        "Number of Fisher Scoring iterations: 4\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We see that `R` produces a standard error for each parameter we estimated. \n",
      "(Where does this come from?)\n",
      "\n",
      "For completeness, we show that using the Newton-Raphson algorithm above\n",
      "yields the same estimates of coefficients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%R -o X,counts,C,SE"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Here is a Poisson regression model in 30 or so lines (excluding comments...)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class poisson(object):\n",
      "    \n",
      "    def __init__(self, X, Y):\n",
      "        self.X = X\n",
      "        self.Y = Y * 1.\n",
      "        \n",
      "    def value(self, beta):\n",
      "        eta = np.dot(self.X, beta)\n",
      "        return (np.exp(eta) - eta*self.Y).sum()\n",
      "    \n",
      "    def grad(self, beta):\n",
      "        eta = np.dot(self.X, beta)\n",
      "        mu = np.exp(eta)\n",
      "        return np.dot(self.X.T, mu - self.Y)\n",
      "    \n",
      "    def hess(self, beta):\n",
      "        eta = np.dot(self.X, beta)\n",
      "        mu = np.exp(eta)\n",
      "        W = mu\n",
      "        X = self.X\n",
      "        return np.dot(X.T, W[:,np.newaxis] * X)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's instantiate the model and verify we've computed our gradient correctly. If correct,\n",
      "the vector `C` should have gradient approximately 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = poisson(X, counts)\n",
      "model.grad(C)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([ 0.0000001 , -0.00000002,  0.00000003, -0.00000002,  0.00000003,\n",
        "       -0.00000002,  0.00000002, -0.00000001])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As for the Hessian, the diagonal diagonal of its inverse should yield the standard errors. (Why?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H = model.hess(C)\n",
      "print 'Hessian inverse:', np.sqrt(np.diag(np.linalg.inv(H)))\n",
      "print 'SE:', SE"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hessian inverse: [ 0.03435823  0.35327947  0.35132316  0.32856422  0.30931663  0.30917959\n",
        "  0.20410041  0.20382491]\n",
        "SE: [ 0.03435753  0.35327081  0.35131134  0.3285515   0.30930698  0.30917085\n",
        "  0.2040974   0.20382201]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Now, let us fit the model using Newton-Raphson."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = np.ones(X.shape[1])\n",
      "value = model.value(beta)\n",
      "\n",
      "for _ in range(10):\n",
      "    step = 1.\n",
      "    count = 0\n",
      "    while True:\n",
      "        proposed_beta = beta - step * np.dot(np.linalg.pinv(model.hess(beta)), model.grad(beta))\n",
      "        if model.value(proposed_beta) > value:\n",
      "            step *= 0.5\n",
      "        else:\n",
      "            break\n",
      "    beta = proposed_beta\n",
      "    value = model.value(beta)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If we've converged, the gradient should be zero."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.grad(beta)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "array([ 0.,  0., -0., -0.,  0.,  0., -0., -0.])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'R:', C\n",
      "print 'Newton-Raphson:', beta"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "R: [  3.89067037  -0.19805055 -10.89545629  -0.14374003   1.99021697\n",
        "   0.01894121   0.31586279   0.07490286]\n",
        "Newton-Raphson: [  3.89067037  -0.19805055 -10.8954563   -0.14374002   1.99021696\n",
        "   0.01894122   0.31586278   0.07490286]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}
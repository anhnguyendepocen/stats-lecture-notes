{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topics covered\n",
    "\n",
    "* Simple linear regression.\n",
    "* Diagnostics for simple linear regression.\n",
    "* Multiple linear regression.\n",
    "* Diagnostics.\n",
    "* Interactions and ANOVA.\n",
    "* Weighted Least Squares.\n",
    "* Autocorrelation.\n",
    "* Model selection.\n",
    "* Penalized regression.\n",
    "* Logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%R -o M,D,slope,intercept\n",
    "library(alr3)\n",
    "data(heights)\n",
    "M = heights$Mheight\n",
    "D = heights$Dheight\n",
    "parameters = lm(D~M)$coef\n",
    "print(parameters)\n",
    "intercept = parameters[1]\n",
    "slope = parameters[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import mlab\n",
    "heights_fig = plt.figure(figsize=(10,6))\n",
    "axes = heights_fig.gca()\n",
    "axes.scatter(M, D, c='red')\n",
    "axes.set_xlabel(\"Mother's height (inches)\", size=20)\n",
    "axes.set_ylabel(\"Daughter's height (inches)\", size=20)\n",
    "X = 66\n",
    "xf, yf = mlab.poly_between([X-.5,X+.5], [50,50], [75, 75])\n",
    "selected_points = (M <= X+.5) * (M >= X-.5)\n",
    "mean_within_slice = D[selected_points].mean()\n",
    "scatterplot_slice = axes.fill(xf, yf, facecolor='blue', alpha=0.1, hatch='/')[0]\n",
    "axes.scatter([X],[mean_within_slice], s=130, c='yellow', marker='^')\n",
    "axes.plot([M.min(), M.max()], [intercept + slope * M.min(), intercept + slope * M.max()],\n",
    "          linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "heights_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Least squares\n",
    "\n",
    "* We used \"least squares\" regression. This measures the goodness of fit of a line by the sum of squared errors, $SSE$.\n",
    "* Least squares regression chooses the line that minimizes $SSE(\\beta_0, \\beta_1) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 \\cdot X_i)^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometry of Least Squares: Simple Linear Model\n",
    "\n",
    "<img src=\"figs/axes_simple.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### What is a $t$-statistic?\n",
    "\n",
    "* Start with $Z \\sim N(0,1)$ is standard normal and $S^2 \\sim \\chi^2_{\\nu}$, independent of $Z$.\n",
    "* Compute $T = \\frac{Z}{\\sqrt{\\frac{S^2}{\\nu}}}.$\n",
    "* Then, $T \\sim t_{\\nu}$ has a $t$-distribution with $\\nu$ degrees of freedom.\n",
    "* Generally, a $t$-statistic has the form $$ T = \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Interval for $\\beta_1$\n",
    "\n",
    " A $(1-\\alpha) \\cdot 100 \\%$ confidence interval: $\\widehat{\\beta}_1 \\pm SE(\\widehat{\\beta}_1) \\cdot t_{n-2, 1-\\alpha/2}.$\n",
    "Interval for regression line $\\beta_0 + \\beta_1 \\cdot X$\n",
    "\n",
    "* $(1-\\alpha) \\cdot 100 \\%$ confidence interval: $\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\pm SE(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X) \\cdot t_{n-2, 1-\\alpha/2}$ where $SE(a_0\\widehat{\\beta}_0 + a_1\\widehat{\\beta}_1) = \\widehat{\\sigma} \\sqrt{\\frac{a_0^2}{n} + \\frac{(a_0\\overline{X} - a_1)^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction intervals\n",
    "\n",
    "* $SE(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_{\\text{new}} + \\varepsilon_{\\text{new}}) = \\widehat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(\\overline{X} - X_{\\text{new}})^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}.$\n",
    "* Prediction interval is $\\widehat{\\beta}_0 +  \\widehat{\\beta}_1 X_{\\text{new}} \\pm t_{n-2, 1-\\alpha/2} \\cdot SE(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_{\\text{new}} + \\varepsilon_{\\text{new}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Sums of squares\n",
    "\n",
    " $$\\begin{aligned}\n",
    "   SSE &= \\sum_{i=1}^n(Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
    "   SSR &= \\sum_{i=1}^n(\\overline{Y} - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (\\overline{Y} - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
    "   SST &= \\sum_{i=1}^n(Y_i - \\overline{Y})^2 = SSE + SSR \\\\\n",
    "   R^2 &= \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = \\widehat{Cor}(\\pmb{X},\\pmb{Y})^2.\n",
    "   \\end{aligned}$$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $F$-test in simple linear regression\n",
    "\n",
    "* *Full (bigger) model :*\n",
    "   $FM: \\qquad Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$\n",
    "* *Reduced (smaller) model:*\n",
    "   $RM: \\qquad Y_i = \\beta_0  + \\varepsilon_i$\n",
    "* The $F$-statistic has the form $F=\\frac{(SSE(RM) - SSE(FM)) / (df_{RM} - df_{FM})}{SSE(FM) / df_{FM}}.$\n",
    "* Reject $H_0: RM$ is correct, if $F > F_{1-\\alpha, 1, n-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Assumptions in the simple linear regression model\n",
    "\n",
    "* $Y_i = \\beta_0 + \\beta_1 X_{i} + \\varepsilon_i$\n",
    "* Errors $\\varepsilon_i$ are assumed independent $N(0,\\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "simple.lm = lm(y2 ~ x2, data=anscombe)\n",
    "plot(anscombe$x2, resid(simple.lm), ylab='Residual', xlab='X',\n",
    "     pch=23, bg='orange', cex=2)\n",
    "abline(h=0, lwd=2, col='red', lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "par(mfrow=c(2,2))\n",
    "plot(simple.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "quadratic.lm <- lm(y2 ~ poly(x2, 2), data=anscombe)\n",
    "Xsort <- sort(anscombe$x2)\n",
    "plot(anscombe$x2, anscombe$y2, pch=23, bg='orange', cex=2, ylab='Y', xlab='X')\n",
    "lines(Xsort, predict(quadratic.lm, list(x2=Xsort)), col='red', lty=2, lwd=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "par(mfrow=c(2,2))\n",
    "plot(quadratic.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple linear diagnostics\n",
    "\n",
    "- Outliers\n",
    "\n",
    "- Nonconstant variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "url = 'http://stats191.stanford.edu/data/HIV.VL.table'\n",
    "\n",
    "viral.load = read.table(url, header=T)\n",
    "attach(viral.load)\n",
    "\n",
    "plot(GSS, VL, pch=23, bg='orange', cex=2)\n",
    "viral.lm = lm(VL ~ GSS)\n",
    "abline(viral.lm, col='red', lwd=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple linear regression model\n",
    "\n",
    "* Rather than one predictor, we have $p=6$ predictors.\n",
    "* $Y_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\varepsilon_i$\n",
    "* Errors $\\varepsilon$ are assumed independent $N(0,\\sigma^2)$, as in simple linear regression.\n",
    "* Coefficients are called (partial) regression coefficients because they \"allow\" for the effect of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometry of Least Squares: Multiple Regression\n",
    "\n",
    "<img src=\"figs/axes_multiple.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overall $F$-test\n",
    "\n",
    "* *Full (bigger) model :*\n",
    "   $$Y_i = \\beta_0 + \\beta_1 X_{i1} + \\dots \\beta_p X_{ip} + \\varepsilon_i$$\n",
    "* *Reduced (smaller) model:*\n",
    "   $$Y_i = \\beta_0  + \\varepsilon_i$$\n",
    "* The $F$-statistic has the form $F=\\frac{(SSE(R) - SSE(F)) / (df_R - df_F)}{SSE(F) / df_F}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix formulation\n",
    "\n",
    "* ${ Y}_{n \\times 1} = {X}_{n \\times (p + 1)} {\\beta}_{(p+1) \\times 1} + {\\varepsilon}_{n \\times 1}$\n",
    "* ${X}$ is called the *design matrix*\n",
    "   of the model\n",
    "* ${\\varepsilon} \\sim N(0, \\sigma^2 I_{n \\times n})$ is multivariate normal\n",
    "$SSE$ in matrix form\n",
    "$$SSE(\\beta) = ({Y} - {X} {\\beta})'({Y} - {X} {\\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### OLS estimators\n",
    "\n",
    "* Normal equations yield\n",
    "$$\\widehat{\\beta} = ({X}^t{X})^{-1}{X}^t{Y}\n",
    "$$\n",
    "* Properties: $$\\hat{\\beta}  \\sim N(\\beta, \\sigma^2 (X^TX)^{-1} )$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval for $\\sum_{j=0}^p a_j \\beta_j$\n",
    "\n",
    "* Suppose we want a $(1-\\alpha)\\cdot 100\\%$ CI for $\\sum_{j=0}^p a_j\\beta_j$.\n",
    "* Just as in simple linear regression:\n",
    "* $\\sum_{j=0}^p a_j \\widehat{\\beta}_j \\pm t_{1-\\alpha/2, n-p-1} \\cdot SE\\left(\\sum_{j=0}^p a_j\\widehat{\\beta}_j\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General $F$-tests\n",
    "\n",
    "-   Given two models $R \\subset F$ (i.e. $R$ is a subspace of $F$), we\n",
    "    can consider testing $$ H_0: H_0: \\text{$R$ is adequate (i.e. $\\mathbb{E}(Y) \\in R$)} $$ vs. $$ H_a: \\text{$F$ is adequate (i.e. $\\mathbb{E}(Y) \\in F$)}\n",
    "    $$\n",
    "    \n",
    "    - The test statistic is $$ F = F = \\frac{(SSE(R) - SSE(F)) / (df_R - df_F)}{SSE(F)/df_F} $$\n",
    "\n",
    "-   If $H_0$ is true, $F \\sim F_{df_R-df_F, df_F}$ so we reject $H_0$ at\n",
    "    level $\\alpha$ if $F > F_{df_R-df_F, df_F, 1-\\alpha}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagnostics: What can go wrong?\n",
    "\n",
    "-   Regression function can be wrong: maybe regression function should\n",
    "    have some other form (see diagnostics for simple linear regression).\n",
    "\n",
    "-   Model for the errors may be incorrect:\n",
    "\n",
    "    -   may not be normally distributed.\n",
    "\n",
    "    -   may not be independent.\n",
    "\n",
    "    -   may not have the same variance.\n",
    "\n",
    "-   Detecting problems is more *art* then *science*, i.e. we cannot\n",
    "    *test* for all possible problems in a regression model.\n",
    "\n",
    "-   Basic idea of diagnostic measures: if model is correct then\n",
    "    residuals $e_i = Y_i -\\widehat{Y}_i, 1 \\leq i \\leq n$ should look\n",
    "    like a sample of (not quite independent) $N(0, \\sigma^2)$ random\n",
    "    variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "url = 'http://stats191.stanford.edu/data/scottish_races.table'\n",
    "races.table = read.table(url, header=T)\n",
    "attach(races.table)\n",
    "races.lm = lm(Time ~ Distance + Climb)\n",
    "par(mfrow=c(2,2))\n",
    "plot(races.lm, pch=23 ,bg='orange',cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagnostics measures\n",
    "\n",
    "* DFFITS: $$DFFITS_i = \\frac{\\widehat{Y}_i - \\widehat{Y}_{i(i)}}{\\widehat{\\sigma}_{(i)} \\sqrt{H_{ii}}}$$\n",
    "\n",
    "* Cook's Distance: $$D_i = \\frac{\\sum_{j=1}^n(\\widehat{Y}_j - \\widehat{Y}_{j(i)})^2}{(p+1) \\, \\widehat{\\sigma}^2}$$\n",
    "\n",
    "* DFBETAS: $$DFBETAS_{j(i)} = \\frac{\\widehat{\\beta}_j - \\widehat{\\beta}_{j(i)}}{\\sqrt{\\widehat{\\sigma}^2_{(i)} (X^TX)^{-1}_{jj}}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "influence.measures(races.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outliers\n",
    "\n",
    "* Observations $(Y, X_1, \\dots, X_p)$ that do not follow the model, while most other observations seem to follow the model.\n",
    "* One solution: Bonferroni correction, threshold at $t_{1 - \\alpha/(2*n), n-p-2}$.\n",
    "* Bonferroni: if we are doing many $t$ (or other) tests, say $m >>1$ we can control overall false positive rate at $\\alpha$ by testing each one at level $\\alpha/m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(car)\n",
    "outlierTest(races.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Qualitative variables and interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "url = 'http://stats191.stanford.edu/data/jobtest.table'\n",
    "jobtest.table <- read.table(url, header=T)\n",
    "jobtest.table$ETHN <- factor(jobtest.table$ETHN)\n",
    "attach(jobtest.table)\n",
    "plot(TEST, JPERF, type='n')\n",
    "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
    "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "jobtest.lm1 <- lm(JPERF ~ TEST, jobtest.table)\n",
    "print(summary(jobtest.lm1))\n",
    "plot(TEST, JPERF, type='n')\n",
    "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
    "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')\n",
    "abline(jobtest.lm1$coef, lwd=3, col='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "jobtest.lm4 = lm(JPERF ~ TEST * ETHN)\n",
    "print(summary(jobtest.lm4))\n",
    "plot(TEST, JPERF, type='n')\n",
    "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
    "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')\n",
    "abline(jobtest.lm4$coef['(Intercept)'], jobtest.lm4$coef['TEST'], lwd=3, col='purple')\n",
    "abline(jobtest.lm4$coef['(Intercept)'] + jobtest.lm4$coef['ETHN1'],\n",
    "      jobtest.lm4$coef['TEST'] + jobtest.lm4$coef['TEST:ETHN1'], lwd=3, col='green')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ANOVA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipy_table import *\n",
    "anova_oneway = [('Source', 'SS', 'df', r'$\\mathbb{E}(MS)$'), \n",
    "                ('Treatment', r'$SSTR=\\sum_{i=1}^r n_i \\left(\\overline{Y}_{i\\cdot} - \\overline{Y}_{\\cdot\\cdot}\\right)^2$', \n",
    "                'r-1', r'$\\sigma^2 + \\frac{\\sum_{i=1}^r n_i \\alpha_i^2}{r-1}$'),\n",
    "                ('Error', r'$SSE=\\sum_{i=1}^r \\sum_{j=1}^{n_i}(Y_{ij} - \\overline{Y}_{i\\cdot})^2$',\n",
    "                 r'$\\sum_{i=1}^r (n_i - 1)$', r'$\\sigma^2$')]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "make_table(anova_oneway)   \n",
    "apply_theme('basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ANOVA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "anova_twoway = [('Source', 'SS', '  df  ', r'$\\mathbb{E}(MS)$'), \n",
    "                ('A', r'$SSA=nm\\sum_{i=1}^r  \\left(\\overline{Y}_{i\\cdot\\cdot} - \\overline{Y}_{\\cdot\\cdot\\cdot}\\right)^2$',\n",
    "                 'r-1', r'$\\sigma^2 + nm\\frac{\\sum_{i=1}^r \\alpha_i^2}{r-1}$'),\n",
    "                ('B', r'$SSB=nr\\sum_{j=1}^m  \\left(\\overline{Y}_{\\cdot j\\cdot} - \\overline{Y}_{\\cdot\\cdot\\cdot}\\right)^2$',\n",
    "                 'm-1', r'$\\sigma^2 + nr\\frac{\\sum_{j=1}^m \\beta_j^2}{m-1}$'),\n",
    "                ('A:B', \n",
    "                 r'$SSAB = n\\sum_{i=1}^r \\sum_{j=1}^m  \\left(\\overline{Y}_{ij\\cdot} - \\overline{Y}_{i\\cdot\\cdot} - \\overline{Y}_{\\cdot j\\cdot} + \\overline{Y}_{\\cdot\\cdot\\cdot}\\right)^2$',\n",
    "                r'(m-1)(r-1)', r'$\\sigma^2 + n\\frac{\\sum_{i=1}^r\\sum_{j=1}^m (\\alpha\\beta)_{ij}^2}{(r-1)(m-1)}$'),\n",
    "                ('Error', r'$SSE = \\sum_{i=1}^r \\sum_{j=1}^m \\sum_{k=1}^{n}(Y_{ijk} - \\overline{Y}_{ij\\cdot})^2$',\n",
    "                 '(n-1)mr', r'$\\sigma^2$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "make_table(anova_twoway)   \n",
    "apply_theme('basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weighted Least Squares\n",
    "\n",
    "* Weighted Least Squares $$SSE(\\beta, w) = \\sum_{i=1}^n w_i \\left(Y_i - \\beta_0 - \\beta_1 X_i\\right)^2.$$\n",
    "* In general, weights should be like: $$w_i = \\frac{1}{\\text{Var}(\\varepsilon_i)}.$$\n",
    "\n",
    "* WLS estimator:\n",
    "$$\n",
    "\\hat{\\beta}_W = (X^TWX)^{-1}(X^TWY).\n",
    "$$\n",
    "\n",
    "* If weights are ignored standard errors are wrong! \n",
    "\n",
    "* Briefly talked about efficiency of estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlated errors: NASDAQ daily close 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "url = 'http://stats191.stanford.edu/data/nasdaq_2011.csv'\n",
    "nasdaq.data = read.table(url, header=TRUE, sep=',')\n",
    "\n",
    "plot(nasdaq.data$Date, nasdaq.data$Close, xlab='Date', ylab='NASDAQ close',\n",
    "     pch=23, bg='red', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AR(1) noise\n",
    "\n",
    "* Suppose that, instead of being independent, the errors in our model were $\\varepsilon_t = \\rho \\cdot \\varepsilon_{t-1} + \\omega_t, \\qquad -1 < \\rho < 1$ with $\\omega_t \\sim N(0,\\sigma^2)$ independent.\n",
    "* If $\\rho$ is close to 1, then errors are very correlated, $\\rho=0$ is independence.\n",
    "* This is \"Auto-Regressive Order (1)\" noise (AR(1)). Many other models of correlation exist: ARMA, ARIMA, ARCH, GARCH, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correcting for AR(1) \n",
    "\n",
    "* Suppose we know $\\rho$, if we \"whiten\" the data and regressors $$\\begin{aligned}\n",
    "     \\tilde{Y}_{t+1} &= Y_{t+1} - \\rho Y_t, t > 1   \\\\\n",
    "     \\tilde{X}_{(t+1)j} &= X_{(t+1)j} - \\rho X_{tj}, i > 1\n",
    "     \\end{aligned}$$ for $1 \\leq t \\leq n-1$. This model satisfies \"usual\" assumptions, i.e. the errors $\\tilde{\\varepsilon}_t = \\omega_{t+1} = \\varepsilon_{t+1} - \\rho \\cdot \\varepsilon_t$ are independent $N(0,\\sigma^2)$.\n",
    "* For coefficients in new model $\\tilde{\\beta}$, $\\beta_0 = \\tilde{\\beta}_0 / (1 - \\rho)$, $\\beta_j = \\tilde{\\beta}_j.$\n",
    "* Problem: in general, we don’t know $\\rho$, but estimated it.\n",
    "\n",
    "* If correlation structure is ignored standard errors are wrong! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "acf(nasdaq.data$Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model selection criteria\n",
    "\n",
    "* Mallow's $C_p$:\n",
    "$$C_p({\\cal M}) = \\frac{SSE({\\cal M})}{\\widehat{\\sigma}^2} + 2 \\cdot p({\\cal M}) - n.$$\n",
    "* Akaike (AIC) defined as $$AIC({\\cal M}) = - 2 \\log L({\\cal M}) + 2 p({\\cal M})$$ where $L({\\cal M})$ is the maximized likelihood of the model.\n",
    "* Bayes (BIC) defined as $$BIC({\\cal M}) = - 2 \\log L({\\cal M}) + \\log n \\cdot p({\\cal M})$$\n",
    "* Adjusted $R^2$\n",
    "* Stepwise (step\n",
    "  ) vs. best subsets (leaps\n",
    "  )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $K$-fold cross-validation \n",
    "\n",
    "* Fix a model ${\\cal M}$. Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
    "* for (i in 1:K)\n",
    "   Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j,{\\cal M}, G_i}, j \\in G_i$.\n",
    "* Estimate $$CV({\\cal M}) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j,{\\cal M},G_i})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shrinkage estimator\n",
    "\n",
    "* In one sample problem, when trying to estimate $\\mu$ from $Y_i \\sim N(\\mu, \\sigma^2)$ we looked at the estimator\n",
    "$$\n",
    "\\hat{Y}_{\\alpha} = \\alpha \\cdot \\bar{Y}.\n",
    "$$\n",
    "\n",
    "* The \"quality\" of the estimator decomposed as\n",
    "$$\n",
    "E((\\hat{Y}_{\\alpha}-\\mu)^2) = \\text{Bias}(\\hat{Y}_{\\alpha})^2 + \\text{Var}(\\hat{Y}_{\\alpha})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "nsample = 40\n",
    "ntrial = 500\n",
    "mu = 0.5\n",
    "sigma = 2.5\n",
    "MSE <- function(mu.hat, mu) {\n",
    "  return(sum((mu.hat - mu)^2) / length(mu))\n",
    "}\n",
    "\n",
    "alpha <- seq(0.0,1,length=20)\n",
    "\n",
    "mse <- numeric(length(alpha))\n",
    "\n",
    "for (i in 1:ntrial) {\n",
    "  Z = rnorm(nsample) * sigma + mu\n",
    "  for (j in 1:length(alpha)) {\n",
    "    mse[j] = mse[j] + MSE(alpha[j] * mean(Z) * rep(1, nsample), mu * rep(1, nsample)) / ntrial\n",
    "  }\n",
    "}\n",
    "\n",
    "plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
    "     xlab=expression(paste('Penalty parameter,', alpha)), \n",
    "     ylab=expression(paste('MSE(', alpha, ')')), \n",
    "     cex.lab=1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda} = \\text{argmin}_{\\beta} \\|Y-X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "library(lars)\n",
    "data(diabetes)\n",
    "library(MASS)\n",
    "diabetes.ridge <- lm.ridge(diabetes$y ~ diabetes$x, lambda=seq(0,10,0.05))\n",
    "plot(diabetes.ridge, lwd=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "par(cex.lab=2)\n",
    "plot(diabetes.ridge$lambda, diabetes.ridge$GCV, xlab='Lambda', ylab='GCV', type='l', lwd=3, col='orange')\n",
    "select(diabetes.ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LASSO\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda} = \\text{argmin}_{\\beta} \\|Y-X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(lars)\n",
    "data(diabetes)\n",
    "diabetes.lasso = lars(diabetes$x, diabetes$y, type='lasso')\n",
    "plot(diabetes.lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "par(cex.lab=2)\n",
    "cv.lars(diabetes$x, diabetes$y, K=10, type='lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression model\n",
    "\n",
    "* Logistic model $$E(Y|X) = \\pi(X) = \\frac{\\exp(X^T\\beta)}{1 + \\exp(X^T\\beta)}$$\n",
    "* This automatically fixes $0 \\leq E(Y) = \\pi(X) \\leq 1$.\n",
    "* The logistic transform:\n",
    "   $\\text{logit}(\\pi(X)) = \\log\\left(\\frac{\\pi(X)}{1 - \\pi(X)}\\right) = X^T\\beta$\n",
    "   \n",
    "* An example of a *generalized linear model*\n",
    "    - link function $\\text{logit}(\\pi(X)) = X^T\\beta$\n",
    "    - Variance function: $\\text{Var}(Y|X) = \\pi(X)(1 - \\pi(X))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Odds Ratios\n",
    "\n",
    "* One reason logistic models are popular is that the parameters have simple interpretations in terms of odds.\n",
    "* Logistic model: $$OR_{X_j} = \\frac{ODDS(Y=1|\\dots, X_j=x_j+1, \\dots)}{ODDS(Y=1|\\dots, X_j=x_j, \\dots)} = e^{\\beta_j}$$\n",
    "* If $X_j \\in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are $e^{\\beta_j}$ higher, other parameters being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deviance\n",
    "\n",
    "* For logistic regression model ${\\cal M}$, $DEV({\\cal M})$ replaces $SSE({\\cal M})/\\sigma^2$.\n",
    "* In least squares regression, we use $$\\frac{SSE({\\cal M}_R) - SSE({\\cal M}_F)}{\\sigma^2} \\sim  \\chi^2_{df_R-df_F}$$\n",
    "* This is replaced with $DEV({\\cal M}_R) - DEV({\\cal M}_F) \\overset{n \\rightarrow \\infty}{\\sim} \\chi^2_{df_R-df_F}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

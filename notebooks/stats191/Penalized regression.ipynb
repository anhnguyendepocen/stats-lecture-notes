{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias-variance tradeoff \n",
    "\n",
    "- One goal of a regression analysis is to \"build\" a model that predicts well.\n",
    "\n",
    "- This is slightly different than the goal of making inferences about $\\beta$ that we've focused on so far.\n",
    "\n",
    "- What does \"predict well\" mean? \n",
    "$$\n",
    "\\begin{aligned}\n",
    "     MSE_{pop}({{\\cal M}}) &= {\\mathbb{E}}\\left((Y_{new} - \\widehat{Y}_{new,{\\cal M}})^2\\right) \\\\\n",
    "     &=\n",
    "     {\\text{Var}}(Y_{new}) + {\\text{Var}}(\\widehat{Y}_{new,{\\cal M}}) +\n",
    "     \\\\\n",
    "     & \\qquad \\quad \\text{Bias}(\\widehat{Y}_{new,{\\cal M}})^2.\n",
    "     \\end{aligned}$$\n",
    " \n",
    "- With $\\sigma^2$ known, the quantity $C_p({\\cal M})$ is an unbiased estimator\n",
    "of this quantity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shrinkage estimators \n",
    "\n",
    "1. Generate $Y_{100 \\times 1} \\sim N(\\mu \\cdot 1, 5^2 I_{100 \\times 100})$, with $\\mu=0.5$.\n",
    "2. For $0 \\leq \\alpha \\leq 1$, set $\\hat{Y}(\\alpha) = \\alpha \\bar{Y}.$\n",
    "3. Compute $MSE(\\hat{Y}(\\alpha)) = \\sum_{i=1}^{100} (\\hat{Y}_{\\alpha} - 0.5)^2$\n",
    "4. Repeat 500 times, plot average of $MSE(\\hat{Y}(\\alpha))$.\n",
    "\n",
    "**For what value of $\\alpha$ is $\\hat{Y}(\\alpha)$ unbiased?**\n",
    "\n",
    "**Is this the best estimate of $\\mu$ in terms of MSE?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "par(cex.lab=5)\n",
    "nsample = 100\n",
    "ntrial = 500\n",
    "mu = 0.5\n",
    "sigma = 5\n",
    "MSE <- function(mu.hat, mu) {\n",
    "  return(sum((mu.hat - mu)^2) / length(mu))\n",
    "}\n",
    "\n",
    "alpha <- seq(0.0,1,length=20)\n",
    "\n",
    "mse <- numeric(length(alpha))\n",
    "\n",
    "for (i in 1:ntrial) {\n",
    "  Z = rnorm(nsample) * sigma + mu\n",
    "  for (j in 1:length(alpha)) {\n",
    "    mse[j] = mse[j] + MSE(alpha[j] * mean(Z) * rep(1, nsample), mu * rep(1, nsample)) / ntrial\n",
    "  }\n",
    "}\n",
    "\n",
    "plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
    "     xlab=expression(paste('Penalty parameter,', alpha)), \n",
    "     ylab=expression(paste('MSE(', alpha, ')')), \n",
    "     cex.lab=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shrinkage & Penalties\n",
    "\n",
    "* Shrinkage can be thought of as \"constrained\" or \"penalized\" minimization.\n",
    "\n",
    "* Constrained form:\n",
    "$$\\text{minimize}_{\\mu} \\sum_{i=1}^n (Y_i - \\mu)^2 \\quad \\text{subject to $\\mu^2 \\leq C$}$$\n",
    "\n",
    "* Lagrange multiplier form: equivalent to \n",
    "$$\\widehat{\\mu}_{\\lambda} = \\text{argmin}_{\\mu} (Y_i - \\mu)^2 + \\lambda \\cdot \\mu^2$$ for some $\\lambda=\\lambda_C$.\n",
    "\n",
    "* As we vary $\\lambda$ we solve all versions of the constrained form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving for $\\widehat{\\mu}_{\\lambda}$\n",
    "\n",
    "* Differentiating: $- 2 \\sum_{i=1}^n (Y_i - \\widehat{\\mu}_{\\lambda}) + 2 \\lambda \\widehat{\\mu}_{\\lambda} = 0$\n",
    "* Solving $\\widehat{\\mu}_{\\lambda} = \\frac{\\sum_{i=1}^n Y_i}{n + \\lambda} = \\frac{n}{n+\\lambda} \\overline{Y}.$\n",
    "* As $\\lambda \\rightarrow 0$, $\\widehat{\\mu}_{\\lambda} \\rightarrow {\\overline{Y}}.$\n",
    "* As $\\lambda \\rightarrow \\infty$ $\\widehat{\\mu}_{\\lambda} \\rightarrow 0.$\n",
    "\n",
    "** We see that $\\widehat{\\mu}_{\\lambda} = \\bar{Y} \\cdot \\left(\\frac{n}{n+\\lambda}\\right).$ **\n",
    "\n",
    "** In other words, considering all penalized estimators traces out the\n",
    "MSE curve above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "lam = nsample / alpha - nsample\n",
    "plot(lam, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
    "     xlab=expression(paste('Penalty parameter,',  lambda)), \n",
    "     ylab=expression(paste('MSE(', lambda, ')')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How much to shrink?\n",
    "\n",
    "- In our one-sample example,\n",
    "- $$\\begin{aligned}\n",
    " MSE_{pop}(\\alpha) &= {\\text{Var}}( \\alpha \\bar{Y}) + \\text{Bias}(\\alpha \\bar{Y})^2 \\\\\n",
    " &= \\frac{\\alpha^2 \\sigma^2}{n} + \\mu^2 (1 - \\alpha)^2\n",
    " \\end{aligned}$$\n",
    "- Differentiating and solving: \n",
    "$$\\begin{aligned}\n",
    " 0 &= -2 \\mu^2(1 - \\lambda^*) + 2 \\frac{\\lambda^* \\sigma^2}{n}  \\\\\n",
    " \\lambda^* & = \\frac{\\mu^2}{\\mu^2+\\sigma^2/n} \\\\\n",
    " &= \\frac{0.5^2}{0.5^2+1/10} = 0.71\n",
    " \\end{aligned}$$\n",
    "     \n",
    "** We see that the optimal $\\alpha$ depends on the unknown $SNR=\\mu/(\\sigma/\\sqrt{n})$.**\n",
    "\n",
    "** In practice we might hope to estimate MSE with cross-validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how our theoretical choice matches the \n",
    "MSE on our 100 sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
    "     xlab=expression(paste('Shrinkage parameter ', alpha)), \n",
    "     ylab=expression(paste('MSE(', alpha, ')')))\n",
    "abline(v=mu^2/(mu^2+sigma^2/nsample), col='blue', lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Penalties & Priors\n",
    "\n",
    "- Minimizing $\\sum_{i=1}^n (Y_i - \\mu)^2 + \\lambda \\mu^2$ is similar to computing \"MLE\" of $\\mu$ if the likelihood was proportional to \n",
    "$$\\exp \\left(-\\frac{1}{2\\sigma^2}\\left(  \\|Y-\\mu\\|^2_2 + \\lambda \\mu^2\\right) \\right).$$\n",
    "\n",
    "- If $\\lambda=m$, an integer, then $\\widehat{\\mu}_{\\lambda}$ is the sample mean of $(Y_1, \\dots, Y_n,0 ,\\dots, 0) \\in \\mathbb{R}^{n+m}$.\n",
    "\n",
    "- This is equivalent to adding some data with $Y=0$. \n",
    "\n",
    "- To a Bayesian,\n",
    "this extra data is a *prior distribution* and we are computing the so-called\n",
    "*MAP* or posterior mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AIC as penalized regression\n",
    "\n",
    "- Model selection with $C_p$ (or AIC with $\\sigma^2$ assumed known)\n",
    "is a version of penalized regression.\n",
    "\n",
    "- The best subsets version of AIC (which is not exactly equivalent to *step*)\n",
    "$$\n",
    "\\hat{\\beta}_{AIC} = \\text{argmin}_{\\beta} \\frac{1}{\\sigma^2}\\|Y-X\\beta\\|^2_2 + 2 \\|\\beta\\|_0\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\|\\beta\\|_0 = \\#\\left\\{j : \\beta_j \\neq 0 \\right\\}\n",
    "$$\n",
    "is called the $\\ell_0$ norm.\n",
    "\n",
    "- The $\\ell_0$ penalty can be thought of as a measure of *complexity* of the model. Most penalties are similar versions of *complexity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Penalized regression in general\n",
    "\n",
    "* Not all biased models are better – we need a way to find \"good\" biased models.\n",
    "\n",
    "* Inference ($F$, $\\chi^2$ tests, etc) is not quite exact for biased models.\n",
    "Though, there has been some recent work to address the issue of [post-selection inference](http://arxiv.org/abs/1311.6238), at least for some penalized regression problems.\n",
    "\n",
    "* Heuristically, \"large $\\beta$\" (measured by some norm) is interpreted as \"complex model\". Goal is really to penalize \"complex\" models, i.e. Occam’s razor.\n",
    "* If truth really is complex, this may not work! (But, it will then be hard to build a good model anyways ... (statistical lore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression\n",
    "\n",
    "- Assume that columns $(X_j)_{1 \\leq j \\leq p}$ have zero mean, and length 1 and $Y$ has zero mean.\n",
    "\n",
    "- This is called the *standardized model*.\n",
    "\n",
    "- The ridge estimator is\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda} = \\text{argmin}_{\\beta} = \\|Y-X\\beta\\|^2_2 + \\lambda \\|\\beta\\|^2_2$$\n",
    "  \n",
    "- Corresponds (through Lagrange multiplier) to a quadratic constraint on ${\\beta_{}}$’s.\n",
    "\n",
    "- This is the natural generalization of the penalized\n",
    "version of our shrinkage estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving the normal equations\n",
    "\n",
    "* Normal equations $$\\frac{\\partial}{\\partial {\\beta_{l}}} SSE_{\\lambda}({\\beta_{}}) = -2  \\langle Y - X{\\beta_{}}, X_l \\rangle + 2 \\lambda {\\beta_{l}}$$\n",
    "* $$-2 \\langle Y - X{\\widehat{\\beta}_{\\lambda}}, X_l \\rangle + 2 \\lambda {\\widehat{\\beta}_{l,\\lambda}} = 0, \\qquad 1 \\leq l \\leq p$$\n",
    "* In matrix form $$-X^TY +  (X^TX + \\lambda I) {\\widehat{\\beta}_{\\lambda}} = 0.$$\n",
    "* Or $${\\widehat{\\beta}_{\\lambda}} = (X^TX + \\lambda I)^{-1} X^TY.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "library(lars)\n",
    "data(diabetes)\n",
    "library(MASS)\n",
    "diabetes.ridge <- lm.ridge(diabetes$y ~ diabetes$x, lambda=seq(0,10,0.05))\n",
    "plot(diabetes.ridge, lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing $\\lambda$\n",
    "\n",
    "* If we knew $MSE$ as a function of $\\lambda$ then we would simply choose the $\\lambda$ that minimizes $MSE$.\n",
    "* To do this, we need to estimate $MSE$.\n",
    "* A popular method is cross-validation as a function of $\\lambda$. Breaks the data up into smaller groups and uses part of the data to predict the rest.\n",
    "* We saw this in diagnostics (Cook’s distance measured the fit with and without each point in the data set) and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $K$-fold cross-validation for penalized model\n",
    "\n",
    "* Fix a model (i.e. fix $\\lambda$). Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
    "* for (i in 1:K)\n",
    "   Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j(i),\\lambda}, j \\in G_i$.\n",
    "* Estimate $CV(\\lambda) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j(i),\\lambda})^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a function to estimate the CV for our one parameter example. In practice, we only have one sample to form the CV curve. In this example below,\n",
    "I will compute the average CV error for 500 trials to show that it is roughly\n",
    "comparable in shape to the MSE curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "K = 5\n",
    "CV = function(Z, alpha) {\n",
    "    cve = numeric(K)\n",
    "    l = length(Z)\n",
    "    for (i in 1:K) {\n",
    "        g = c(((i-1)*l/K+1):(i*l/K))\n",
    "        mu.hat = mean(Z[-g]) * alpha\n",
    "        cve[i] = sum((Z[g]-mu.hat)^2)\n",
    "    }\n",
    "    return(c(cve, sd(cve)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how the parameter chosen by 5-fold CV compares to our\n",
    "theoretical choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "\n",
    "alpha = seq(0.0,1,length=20)\n",
    "\n",
    "mse = numeric(length(alpha))\n",
    "avg.cv = numeric(length(alpha)) \n",
    "\n",
    "for (i in 1:ntrial) {\n",
    "     Z = rnorm(nsample) * sigma + mu\n",
    "     for (j in 1:length(alpha)) {\n",
    "         current_cv = CV(Z, alpha[j])\n",
    "         avg.cv[j] = avg.cv[j] + current_cv[1] / ntrial\n",
    "     }\n",
    " }\n",
    "\n",
    "avg.cv = avg.cv / nsample\n",
    "\n",
    " plot(alpha, avg.cv, type='l', lwd=2, col='green',\n",
    "    xlab='Shrinkage parameter, alpha', ylab='Average CV(alpha)')\n",
    " abline(v=mu^2/(mu^2+sigma^2/nsample), col='blue', lty=2)\n",
    " abline(v=min(alpha[avg.cv == min(avg.cv)]), col='red', lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The curve above shows what would happen if we\n",
    "could repeat this and averaeg over many samples.\n",
    "\n",
    "Let's see what one curve looks like on our  sample. This is the result we might\n",
    "get in practice on a given data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "set.seed(0)\n",
    "\n",
    "cv = numeric(length(alpha))\n",
    "cv.sd = numeric(length(alpha))\n",
    "Z = rnorm(nsample) * sigma + mu\n",
    "for (j in 1:length(alpha)) {\n",
    "    current_cv = CV(Z, alpha[j])\n",
    "    cv[j] = current_cv[1]\n",
    "    cv.sd[j] = current_cv[2]\n",
    "}\n",
    "\n",
    "plot(alpha, cv, type='l', lwd=2, col='green',\n",
    "    xlab='Shrinkage parameter, alpha', ylab='CV(alpha)')\n",
    " abline(v=mu^2/(mu^2+sigma^2/nsample), col='blue', lty=2)\n",
    " abline(v=min(alpha[cv == min(cv)]), col='red', lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Cross Validation\n",
    "\n",
    "* A computational shortcut for $n$-fold cross-validation (also known as leave-one out cross-validation).\n",
    "* Let $S_{\\lambda} = S(X^TX + \\lambda I)^{-1} X^T$ be the matrix in ridge regression that computes $\\hat{Y}_{\\lambda}$\n",
    "* Then $GCV(\\lambda) =  \\frac{\\|Y - S_{\\lambda}Y\\|^2}{n - {\\text{Tr}}(S_{\\lambda})}.$\n",
    "* The quantity ${\\text{Tr}}(S_{\\lambda})$ can be thought of as the *effective degrees of freedom* for this choice of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "par(cex.lab=2)\n",
    "plot(diabetes.ridge$lambda, diabetes.ridge$GCV, xlab='Lambda', ylab='GCV', type='l', lwd=3, col='orange')\n",
    "select(diabetes.ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LASSO \n",
    "\n",
    "- Another popular penalized regression technique.\n",
    "- Use the standardized model.\n",
    "- The LASSO estimate is\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda} = \\text{argmin}_{\\beta} \\|Y-X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_1$$\n",
    "where\n",
    "$$\n",
    "\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "is the $\\ell_1$ norm.\n",
    "\n",
    "- Corresponds (through Lagrange multiplier) to an $\\ell^1$ constraint on ${\\beta_{}}$’s. \n",
    "\n",
    "- In theory and practice, it works well when many ${\\beta_{j}}$’s are 0 and gives \"sparse\" solutions unlike ridge.\n",
    "\n",
    "- It is a (computable) approximation to the best subsets AIC model.\n",
    "\n",
    "- It is  computable because the minimization problem is a convex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why do we get sparse solutions with the LASSO?\n",
    "\n",
    "<img src=\"figs/lassofig.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "\n",
    "library(lars)\n",
    "data(diabetes)\n",
    "diabetes.lasso = lars(diabetes$x, diabetes$y, type='lasso')\n",
    "plot(diabetes.lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation for the LASSO\n",
    "\n",
    "The `lars` package has a built in function to estimate CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "par(cex.lab=2)\n",
    "cv.lars(diabetes$x, diabetes$y, K=10, type='lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic Net\n",
    "\n",
    "* Mix between LASSO and ridge regression.\n",
    "* The ENET estimator is\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda_1, \\lambda_2} = \\text{argmin}_{\\beta} \\|Y-X\\beta\\|^2_2 + \\lambda_1 \\|\\beta\\|_1 + \\lambda_2 \\|\\beta\\|^2_2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The beginnings of inference after LASSO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "par(cex.lab=2)\n",
    "library(genlasso)\n",
    "GL = genlasso(diabetes$y, diabetes$x, diag(rep(1,ncol(diabetes$x))))\n",
    "plot(GL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Covariance test for LASSO path\n",
    "\n",
    "* The events $\\lambda_i=\\lambda_i(X,y)$ can be thought of as event \"times\". \n",
    "\n",
    "* They are random variables themselves, maybe we can say something\n",
    "about their distribution?\n",
    "\n",
    "* Suppose $\\text{diag}(X^TX)=1$ (i.e. we rescale columns of design matrix to have constant length)\n",
    "\n",
    "* Then, under $H_0:\\beta=0$\n",
    "$$\n",
    "\\frac{1 - {\\tt pnorm}(\\lambda_1/\\sigma)}{1 - {\\tt pnorm}(\\lambda_2/\\sigma)} \\sim \\text{Unif}(0,1).\n",
    "$$\n",
    "\n",
    "* Can also describe what happens under other alternatives. [Take a look at the paper](http://arxiv.org/abs/1401.3889).\n",
    "\n",
    "* Code coming!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "metadata": {
  "name": "",
  "signature": "sha256:75c56a2146009bdb2a8390f62139637892ef731bd51a5fe3a8c2639b07320e4c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Diagnostics for simple regression\n",
      "\n",
      "* Goodness of fit of regression: analysis of variance.\n",
      "\n",
      "* $F$-statistics.   \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Geometry of least squares\n",
      "\n",
      "Here are three pictures that help to describe different models we might fit.\n",
      "\n",
      "### The full model\n",
      "\n",
      "<img src=\"figs/axes_simple_full.svg\">\n",
      "\n",
      "* This picture is meant to depict the regression model\n",
      "$$\n",
      "Y = \\gamma \\cdot 1 + X \\beta + \\epsilon.\n",
      "$$\n",
      "\n",
      "* The $\\gamma$ coefficient represents movement along the horizontal axis above, labelled $\\pmb{1}$.\n",
      "\n",
      "* The $\\beta$ coefficient represents movement along the axis $X$ above.\n",
      "\n",
      "* The vector $\\hat{Y}$ is the vector of fitted values in the above model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### The reduced model\n",
      "\n",
      "<img src=\"figs/axes_simple_reduced.svg\">\n",
      "\n",
      "* This picture is meant to depict the regression model\n",
      "$$\n",
      "Y = \\gamma \\cdot 1 + \\epsilon.\n",
      "$$\n",
      "\n",
      "* The $\\gamma$ coefficient represents movement along the horizontal axis above, labelled $\\pmb{1}$.\n",
      "\n",
      "* Since $\\beta=0$, we have assumed there is no movement along the $X$ axis.\n",
      "\n",
      "* The vector $\\hat{Y}$ is the vector of fitted values in the above model.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Both models together\n",
      "\n",
      "<img src=\"figs/axes_simple.svg\"> \n",
      "\n",
      "* The above picture tries to capture both models in one image.\n",
      "\n",
      "* There is a new vector: $\\hat{Y} - \\bar{Y} \\cdot \\pmb{1}$. This vector is the difference in fits between the two\n",
      "previous models.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Goodness of fit\n",
      "\n",
      "* The closer $\\hat{Y}$ is to the ${1}$ axis, the less \"variation\" there is along the $X$ axis. \n",
      "\n",
      "* This closeness can be measured by the length of the vector $\\hat{Y}-\\bar{Y} \\cdot 1$.\n",
      "\n",
      "* The square of a vector's length is the sum of its elements squared. These quantities\n",
      "are usually referred to as *sums of squares*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Sums of squares\n",
      "\n",
      "$$\n",
      "\\begin{aligned}\n",
      "   SSE &= \\sum_{i=1}^n(Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
      "   SSR &= \\sum_{i=1}^n(\\overline{Y} - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (\\overline{Y} - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
      "   SST &= \\sum_{i=1}^n(Y_i - \\overline{Y})^2 = SSE + SSR \\\\\n",
      "   R^2 &= \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = \\widehat{Cor}(\\pmb{X},\\pmb{Y})^2.\n",
      "   \\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "\n",
      "* The quantity $SSE$, or *error sum of squares*, is the squared length of the vector $Y -  \\hat{Y}$ which protrudes perpendicular to the plane.\n",
      "\n",
      "* The quantity $SSR$, or *regression sum of squares*, is the length of the vector $\\hat{Y} - \\bar{Y} \\cdot 1$.\n",
      "\n",
      "* The quantity $SST$, or *total sum of squares*, is the length of the vector $Y - \\bar{Y} \\cdot 1$.\n",
      "\n",
      "* The quantity $R^2$ is a measure of the goodness of fit of the simple linear regression model. Values near 1 indicate\n",
      "much of the total variability in $Y$ is explained by the regression model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Mean squares\n",
      "\n",
      "* Each sum of squares gets an extra bit of information associated to them, called their *degrees of freedom*.\n",
      "\n",
      "* Roughly speaking, the *degrees of freedom* can be determined by dimension counting.\n",
      "\n",
      "* The $SSE$ has $n-2$ degrees of freedom  because it is the squared length of a vector that lies in $n-2$ dimensions. To see this, note that it is\n",
      "perpendicular to the 2-dimensional plane formed by the $X$ axis and the $1$ axis.\n",
      "\n",
      "* The $SST$ has $n-1$ degrees of freedom because it is the squared length of a vector that lies in $n-1$ dimensions. In this case, this vector is perpendicular to the $1$ axis.\n",
      "\n",
      "* The $SSR$ has 1 degree of freedom because it is the squared length of a vector that lies in the 2-dimensional plane but is perpendicular to the $1$ axis.\n",
      "\n",
      "$$\n",
      "\\begin{aligned}\n",
      "   MSE &= \\frac{1}{n-2}\\sum_{i=1}^n(Y_i - \\widehat{Y}_i)^2 \\\\\n",
      "   MSR &= \\sum_{i=1}^n(\\overline{Y} - \\widehat{Y}_i)^2 \\\\\n",
      "   MST &= \\frac{1}{n-1}\\sum_{i=1}^n(Y_i - \\overline{Y})^2 \\\\\n",
      "   \\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$F$-statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "After a $t$-statistic or $Z$-statistic, the next most commonly encountered statistic is a $\\chi^2$ statistic, or its closely related cousin,\n",
      "the $F$ statistic.\n",
      "\n",
      "* Roughly speaking, an $F$-statistic is a ratio of *sample variances*: it has a numerator, $N$, \n",
      " and a denominator, $D$ that are independent.\n",
      " \n",
      " *  Let $$N \\sim \\frac{\\chi^2_{\\rm num} }{ df_{{\\rm num}}}, \\qquad D \\sim \\frac{\\chi^2_{\\rm den} }{ df_{{\\rm den}}}$$\n",
      " and define\n",
      " $$\n",
      " F = \\frac{N}{D}.\n",
      " $$\n",
      " \n",
      " * We say $F$ has an $F$ distribution with parameters $df_{{\\rm num}}, df_{{\\rm den}}$ and write $F \\sim F_{df_{{\\rm num}}, df_{{\\rm den}}}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$F$ statistic for simple linear regression\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\n",
      "* The ratio $$\n",
      "   F=\\frac{SSR/1}{SSE/(n-2)} = \\frac{MSR}{MSE}$$\n",
      "   can be thought of as a *ratio of variances*.\n",
      "\n",
      "* In fact, under $H_0:\\beta_1=0$, $$\n",
      "   F \\sim F_{1, n-2}\n",
      "   $$\n",
      "because $$\n",
      "   \\begin{aligned}\n",
      "   SSR &= \\|\\hat{Y} - \\bar{Y} \\cdot 1\\|^2 \\\\\n",
      "   SSE &= \\|Y - \\hat{Y}\\|^2\n",
      "   \\end{aligned}\n",
      "   $$\n",
      "   and from our picture, these vectors are orthogonal.\n",
      "   \n",
      "* The null hypothesis $H_0:\\beta_1=0$ implies that $SSR \\sim \\chi^2_1$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Relation between $F$ and $t$ statistics."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* If $T \\sim t_{\\nu}$, then\n",
      "   $$\n",
      "   T^2 \\sim \\frac{N(0,1)^2}{\\chi^2_{\\nu}/\\nu} \\sim \\frac{\\chi^2_1/1}{\\chi^2_{\\nu}/\\nu}.$$\n",
      "\n",
      "* In other words, the square of a $t$-statistic is an $F$-statistic.\n",
      "   Because it is always positive, an $F$-statistic has no *direction* associated with it.\n",
      " \n",
      "* In fact \n",
      "   $$\n",
      "   F = \\frac{MSR}{MSE} = \\frac{\\widehat{\\beta}_1^2}{SE(\\widehat{\\beta}_1)^2}.$$\n",
      "Let's check this in our example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "summary(wages.lm)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "\n",
        "Call:\n",
        "lm(formula = logwage ~ education, data = wages)\n",
        "\n",
        "Residuals:\n",
        "     Min       1Q   Median       3Q      Max \n",
        "-1.78239 -0.25265  0.01636  0.27965  1.61101 \n",
        "\n",
        "Coefficients:\n",
        "            Estimate Std. Error t value Pr(>|t|)    \n",
        "(Intercept) 1.239194   0.054974   22.54   <2e-16 ***\n",
        "education   0.078600   0.004262   18.44   <2e-16 ***\n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n",
        "\n",
        "Residual standard error: 0.4038 on 2176 degrees of freedom\n",
        "Multiple R-squared: 0.1351,\tAdjusted R-squared: 0.1347 \n",
        "F-statistic:   340 on 1 and 2176 DF,  p-value: < 2.2e-16 \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The $t$ statistic for *education* is the $t$-statistic for the parameter $\\beta_1$ under $H_0:\\beta_1=0$. Its value\n",
      "is 18.4 above. If we square it, we should get about the same as the *F-statistic*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "18.44**2"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "340.03360000000004"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Interpretation of an $F$-statistic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* In regression, the numerator is usually a difference in *goodness of fit* of two  (nested) models.\n",
      "\n",
      "* The denominator is $\\hat{\\sigma}^2$ -- an estimate of $\\sigma^2$.\n",
      "\n",
      "* In our example today: the bigger model is the simple linear regression model, the smaller is the model\n",
      " with constant mean (one sample model).\n",
      "\n",
      "* If the $F$ is large, it says that the *bigger*  model explains a lot more variability in $Y$  (relative to $\\sigma^2$) than the smaller one.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The $F$-statistic for simple linear regression revisited"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The $F$ statistic should compare two models. What are these models?\n",
      "\n",
      "* The *full model* would be\n",
      "$$\n",
      "(FM) \\qquad  Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i \n",
      "$$\n",
      "\n",
      "* The *reduced model* would be\n",
      "$$\n",
      "(RM) \\qquad  Y_i = \\beta_0 + \\varepsilon_i \n",
      "$$\n",
      "\n",
      "* The $F$-statistic then has the form\n",
      "$$\n",
      "F=\\frac{(SSE(RM) - SSE(FM)) / (df_{RM} - df_{FM})}{SSE(FM) / df_{FM}}\n",
      "$$\n",
      "\n",
      "* The *null hypothesis* is \n",
      "$$\n",
      "H_0: \\text{model (RM) is correct}.\n",
      "$$\n",
      "\n",
      "* The usual $\\alpha$ rejection rule would be to reject $H_0$ if the $F_{\\text{obs}}$ the  observed $F$ statistic is greater than\n",
      "$F_{1,n-2,1-\\alpha}$. \n",
      "\n",
      "* In our case, the observed $F$ was 340, $n-2=2176$ and the appropriate threshold is computed below to be 3.85. Therefore, we strongly\n",
      "reject $H_0$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "qf(0.95, 1, 2176)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "[1] 3.845736\n"
       ]
      }
     ],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}
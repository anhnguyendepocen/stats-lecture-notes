{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model selection\n",
    "\n",
    "In a given regression situation, there are often many choices to\n",
    "be made. Recall our usual setup\n",
    "$$\n",
    "Y_{n \\times 1} = X_{n \\times (p+1)} \\beta_{(p+1) \\times 1} + \\epsilon_{n \\times 1}.\n",
    "$$\n",
    "\n",
    "Any *subset $A \\subset \\{0, \\dots, p\\}$* yields a new regression model\n",
    "$$\n",
    "{\\cal M}(A): Y_{n \\times 1} = X[,A] \\beta[A] + \\epsilon_{n \\times 1}\n",
    "$$\n",
    "by setting $\\beta[A^c]=0$.\n",
    "\n",
    "**Model selection** is, roughly speaking, how to choose $A$ among the\n",
    "$2^p$ possible choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Election data\n",
    "\n",
    "Here is a dataset from the book that we will use to explore different model selection approaches.\n",
    "\n",
    "Variable | Description\n",
    "--- | ---\n",
    "$V$ | votes for a presidential candidate\n",
    "$I$ | are they incumbent?\n",
    "$D$ | Democrat or Republican incumbent?\n",
    "$W$ | wartime election?\n",
    "$G$ | GDP growth rate in election year\n",
    "$P$ | (absolute) GDP deflator growth rate\n",
    "$N$ | number of quarters in which GDP growth rate $> 3.2\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://stats203.stanford.edu/data/election.table'\n",
    "election.table = read.table(url, header=T)\n",
    "pairs(election.table[,2:ncol(election.table)], cex.labels=3, pch=23,\n",
    "      bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem & Goals\n",
    "\n",
    "* When we have many predictors (with many possible interactions), it can be difficult to find a good model.\n",
    "* Which main effects do we include?\n",
    "* Which interactions do we include?\n",
    "* Model selection procedures try to *simplify / automate* this task.\n",
    "* Election data has $2^6=64$ different models with just main effects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General comments\n",
    "\n",
    "- This is generally an \"unsolved\" problem in statistics: there are no magic procedures to get you the \"best model.\"\n",
    "\n",
    "- In some sense, model selection is \"data mining.\"\n",
    "\n",
    "- Data miners / machine learners often work with very many predictors.\n",
    "\n",
    "- Our model selection problem is generally at a much smaller scale than \"data mining\" problems.\n",
    "\n",
    "- Still, it is a hard problem.\n",
    "\n",
    "- **Inference after selection is full of pitfalls!** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothetical example\n",
    "* Suppose we fit a a model $$F: \\quad Y_{n \\times 1} = X_{n \\times (p+1)} \\beta_{(p+1) \\times 1} + \\varepsilon_{n \\times 1}$$ with predictors ${ X}_1, \\dots, { X}_p$.\n",
    "* In reality, some of the $\\beta$’s may be zero. Let’s suppose that $$\\beta_{j+1}= \\dots= \\beta_{p}=0$$\n",
    "* Then, any model that includes $\\beta_0, \\dots, \\beta_j$ is *correct*: which model gives the *best* estimates of $\\beta_0, \\dots, \\beta_j$?\n",
    "* Principle of *parsimony* (i.e. Occam’s razor) says that the model with *only* ${X}_1, \\dots, {X}_j$ (and an intercept) is \"best\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Justifying parsimony\n",
    "\n",
    "- For simplicity, let’s assume that $j=1$ so there is only one coefficient to estimate.\n",
    "- Then, because each model gives an *unbiased* estimate of $\\beta_1$ we can compare models based on $\\text{Var}(\\widehat{\\beta}_1).$\n",
    "- The best model, in terms of this variance, is the one containing only ${ X}_1$.\n",
    "- What if we didn’t know that only $\\beta_1$ was non-zero (which we don’t know in general)?\n",
    "- In this situation, we must choose a set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model selection: choosing a subset of variables\n",
    "\n",
    "* To \"implement\" a model selection procedure, we first need a criterion or benchmark to compare two models.\n",
    "* Given a criterion, we also need a search strategy.\n",
    "* With a limited number of predictors, it is possible to search all possible models (`leaps` in `R`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Candidate criteria\n",
    "\n",
    "Possible criteria:\n",
    "\n",
    "* $R^2$: not a good criterion. Always increase with model size $\\implies$ \"optimum\" is to take the biggest model.\n",
    "* Adjusted $R^2$: better. It \"penalized\" bigger models. Follows principle of parsimony / Occam’s razor.\n",
    "* Mallow’s $C_p$ – attempts to estimate a model’s predictive power, i.e. the power to predict a new observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best subsets, $R^2$\n",
    "\n",
    "Leaps takes a design matrix as argument: throw away the intercept\n",
    "column or leaps will complain.\n",
    "\n",
    "Recall\n",
    "$$\n",
    "R^2({\\cal M}) = 1 - \\frac{SSE({\\cal M})}{SST}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.lm = lm(V ~ I + D + W +G:I + P + N, election.table)\n",
    "election.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = model.matrix(election.lm)[,-1]\n",
    "library(leaps)\n",
    "election.leaps = leaps(X, election.table$V, nbest=3, method='r2')\n",
    "best.model.r2 = election.leaps$which[which((election.leaps$r2 == max(election.leaps$r2))),]\n",
    "best.model.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's plot the $R^2$ as a function of the model size. We see that the\n",
    "full model does include all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$size, election.leaps$r2, pch=23, bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best subsets, adjusted $R^2$\n",
    "\n",
    "-   As we add more and more variables to the model – even random ones,\n",
    "    $R^2$ will increase to 1.\n",
    "\n",
    "-   Adjusted $R^2$ tries to take this into account by replacing sums of squares by *mean squares*\n",
    "    $$R^2_a({\\cal M}) = 1 - \\frac{SSE({\\cal M})/(n-p({\\cal M}))}{SST/(n-1)} = 1 - \\frac{MSE}{MST}.$$\n",
    "    \n",
    "- Above, $p({\\cal M})$ is the number of columns in the design matrix for the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.leaps = leaps(X, election.table$V, nbest=3, method='adjr2')\n",
    "plot(election.leaps$size, election.leaps$adjr2, pch=23, bg='orange', cex=2)\n",
    "best.model.adjr2 = election.leaps$which[which((election.leaps$adjr2 == max(election.leaps$adjr2))),]\n",
    "best.model.adjr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mallow’s $C_p$\n",
    "\n",
    "- $C_p({\\cal M}) = \\frac{SSE({\\cal M})}{\\widehat{\\sigma}^2} + 2 \\cdot p({\\cal M}) - n.$\n",
    "- $\\widehat{\\sigma}^2=SSE(F)/df_F$ is the \"best\" estimate of $\\sigma^2$ we have (use the fullest model), i.e. in the election data it uses all 6 main effects.\n",
    "- $SSE({\\cal M})$ is the $SSE$ of the model ${\\cal M}$.\n",
    "- $p({\\cal M})$ is the number of predictors in ${\\cal M}$.\n",
    "- This is an estimate of the expected mean-squared error of $\\widehat{Y}({\\cal M})$, it takes *bias* and *variance* into account.\n",
    "- If $\\sigma^2$ were known\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[C_p({\\cal M})] &= E\\left[\\|\\hat{Y}_{\\cal M} - X\\beta\\|^2\\right] \\\\\n",
    "&= E\\left[\\|\\hat{Y}_{\\cal M} - E(\\hat{Y}_{\\cal M})\\|^2\\right] + \\|E(\\hat{Y}_{\\cal M}) - X\\beta\\|^2.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')\n",
    "plot(election.leaps$size, election.leaps$Cp, pch=23, bg='orange', cex=2)\n",
    "best.model.Cp = election.leaps$which[which((election.leaps$Cp == min(election.leaps$Cp))),]\n",
    "best.model.Cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Search strategies \n",
    "\n",
    "* Given a criterion, we now have to decide how we are going to search through the possible models.\n",
    "\n",
    "* \"Best subset\": search all possible models and take the one with highest $R^2_a$ or lowest $C_p$ leaps. Such searches are typically\n",
    "feasible only up to $p=30$ or $40$.\n",
    "\n",
    "* Stepwise (forward, backward or both): useful when the number of predictors is large. Choose an initial model and be \"greedy\".\n",
    "\n",
    "* \"Greedy\" means always take the biggest jump (up or down) in your selected criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Implementations in `R`\n",
    "\n",
    "* \"Best subset\": use the function `leaps`. Works only for multiple linear regression models.\n",
    "* Stepwise: use the function step. Works for any model with Akaike Information Criterion (AIC). In multiple linear regression, AIC is (almost) a linear function of $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Akaike / Bayes Information Criterion\n",
    "\n",
    "* Akaike (AIC) defined as $$AIC({\\cal M}) = - 2 \\log L({\\cal M}) + 2 \\cdot p({\\cal M})$$ where $L({\\cal M})$ is the maximized likelihood of the model.\n",
    "* Bayes (BIC) defined as $$BIC({\\cal M}) = - 2 \\log L({\\cal M}) + \\log n \\cdot p({\\cal M})$$\n",
    "* Strategy can be used for whenever we have a likelihood, so this generalizes to many statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AIC for regression\n",
    "\n",
    "* In linear regression with unknown $\\sigma^2$ $$-2 \\log L({\\cal M}) = n \\log(2\\pi \\widehat{\\sigma}^2_{MLE}) + n$$ where $\\widehat{\\sigma}^2_{MLE} = \\frac{1}{n} SSE({\\cal M})$\n",
    "* In linear regression with known $\\sigma^2$ $$-2 \\log L({\\cal M}) = n \\log(2\\pi \\sigma^2) + \\frac{1}{\\sigma^2} SSE({\\cal M})$$ so AIC is very much like Mallow’s $C_p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = nrow(X)\n",
    "p = 7 + 1 \n",
    "c(n * log(2*pi*sum(resid(election.lm)^2)/n) + n + 2*p, AIC(election.lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Properties of AIC / BIC\n",
    "* BIC will always choose a model as small or smaller than AIC (if using the same search direction).\n",
    "\n",
    "* As our sample size grows, under some assumptions,\n",
    "it can be shown that\n",
    "     - AIC (best subsets) will (asymptotically) always choose a model that contains the true model, i.e. it won’t leave any variables out.\n",
    "     - BIC (best subsets) will (asymptotically) choose exactly the right model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Election example\n",
    "\n",
    "Let's take a look at `step` in action. Probably the simplest\n",
    "strategy is *forward stepwise* which tries to add one variable at a time, \n",
    "as long as it can find a resulting model whose AIC is better than \n",
    "its current position. \n",
    "\n",
    "When it can make no further additions, it terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.step.forward = step(lm(V ~ 1, election.table), \n",
    "                             list(upper = ~ I + D + W + G:I + P + N), \n",
    "                             direction='forward')\n",
    "summary(election.step.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We notice that although the *full* model we gave it had the interaction `I:G`, the function `step` never tried to use it. This is \n",
    "due to some rules implemented in `step` that do not include an interaction unless both main effects are already in the model. In this case, because neither $I$ nor $G$ were added, the interaction was never considered.\n",
    "\n",
    "In the `leaps` example, we gave the function the design matrix\n",
    "and it did not have to consider interactions: they were already encoded in the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### BIC example\n",
    "\n",
    "The only difference between AIC and BIC is the price paid\n",
    "per variable. This is the argument `k` to `step`. By default `k=2` and for BIC\n",
    "we set `k=log(n)`. If we set `k=0` it will always add variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "election.step.forward.BIC = step(lm(V ~ 1, election.table), \n",
    "                                 list(upper = ~ I + D + W +G:I + P + N), \n",
    "                                 direction='forward', k=log(nrow(X)))\n",
    "summary(election.step.forward.BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward selection\n",
    "\n",
    "Just for fun, let's consider backwards stepwise. This starts at a full\n",
    "model and tries to delete variables.\n",
    "\n",
    "There is also a `direction=\"both\"` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.step.backward = step(election.lm, direction='backward')\n",
    "summary(election.step.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "Yet another model selection criterion is \n",
    "$K$-fold cross-validation.\n",
    "\n",
    "- Fix a model ${\\cal M}$. Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
    "- For (i in 1:K) Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j,{\\cal M}, G_i}, j \\in G_i$.\n",
    "- Similar to what we saw in Cook's distance which is $n$-fold cross validation.\n",
    "- Estimate $CV({\\cal M}) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j,{\\cal M},G_i})^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Comments about cross-validation.\n",
    "\n",
    "* It is a general principle that can be used in other situations to \"choose parameters.\"\n",
    "* Pros (partial list): \"objective\" measure of a model.\n",
    "* Cons (partial list): usual inferential tools are *usually* \"out the window\" (also true for other model selection procedures).\n",
    "* If goal is not really inference about certain specific parameters, it is a reasonable way to compare models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $C_p$ versus 5-fold cross-validation\n",
    "\n",
    "Let's plot our $C_p$ versus the $CV$ score.\n",
    "Keep in mind that there is additional randomness in the $CV$ score\n",
    "due to the random assignments to groups. Below, I have set the\n",
    "random seed so the results stay the same each time I run this\n",
    "chunk of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "library(boot)\n",
    "set.seed(0)\n",
    "election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')\n",
    "V = election.table$V\n",
    "election.leaps$cv = 0 * election.leaps$Cp\n",
    "for (i in 1:nrow(election.leaps$which)) {\n",
    "    subset = c(1:ncol(X))[election.leaps$which[i,]]\n",
    "    if (length(subset) > 1) {\n",
    "       Xw = X[,subset]\n",
    "       wlm = glm(V ~ Xw)\n",
    "       election.leaps$CV[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]\n",
    "    }\n",
    "    else {\n",
    "       Xw = X[,subset[1]]\n",
    "       wlm = glm(V ~ Xw)\n",
    "       election.leaps$CV[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$Cp, election.leaps$CV, pch=23, bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$size, election.leaps$CV, pch=23, bg='orange', cex=2)\n",
    "best.model.Cv = election.leaps$which[which((election.leaps$CV == min(election.leaps$CV))),]\n",
    "best.model.Cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "The model selected depends on the criterion used.\n",
    "\n",
    "<table>\n",
    "<tr><td width=\"50\">Criterion</td><td width=\"230\">Model</td></tr>\n",
    "<tr><td>$R^2$</td><td>~ $ I + D + W +G:I + P + N$</td></tr>\n",
    "<tr><td>$R^2_a$ </td><td> ~ $ I + D + P + N$</td></tr>\n",
    "<tr><td>$C_p$ </td><td> ~ $D+P+N$</td></tr>\n",
    "<tr><td>AIC forward </td><td> ~ $D+P$</td></tr>\n",
    "<tr><td>BIC forward </td><td> ~ $D$</td></tr>\n",
    "<tr><td>AIC backward </td><td> ~ $I + D + N + I:G$</td></tr>\n",
    "<tr><td>5-fold CV </td><td> ~ $ I+W$</td></tr>\n",
    "</table>\n",
    "\n",
    "**The selected model is random!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where we are so far\n",
    "\n",
    "- Many other \"criteria\" have been proposed.\n",
    "- Some work well for some types of data, others for different data.\n",
    "- These criteria (except cross-validation) are not \"direct measures\" of predictive power, though Mallow’s $C_p$ is a step in this direction.\n",
    "- $C_p$ measures the quality of a model based on both *bias* and *variance* of the model. \n",
    "\n",
    "####  *Bias-variance* tradeoff is ubiquitous in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference after selection\n",
    "\n",
    "Each of the above criteria return a model. The `summary` provides\n",
    "$p$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(election.step.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also form confidence intervals. **But, can we trust these intervals or tests!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confint(election.step.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data snooping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nsim = 5000\n",
    "exactP = c()\n",
    "naiveP = c()\n",
    "for (i in 1:nsim) {\n",
    "    Y0 = rnorm(nrow(X))   # data independent of the design, all true coefficients are 0\n",
    "    fs.obj = fs(X, Y0, maxsteps=1, intercept=TRUE, normalize=TRUE, verbose=FALSE)\n",
    "    best_col = X[,fs.obj$action[1]]\n",
    "    best_1sparse.lm = lm(Y0 ~ best_col)\n",
    "    \n",
    "    naiveP = c(naiveP, summary(best_1sparse.lm)$coef[2,4]) # this is the entry in the \n",
    "                                                           # summary with the p-value\n",
    "    \n",
    "    exactP = c(exactP, fsInf(fs.obj, sigma=1)$pv[1])       # this is p-values\n",
    "                                                           # that correct for selection\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(ecdf(naiveP), lwd=3, col='red')\n",
    "abline(0,1, lwd=2, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the type I error of the first test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ecdf(naiveP)(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It turns out that it is possible to modify the\n",
    "usual $t$-test so that we still get valid tests\n",
    "after the first step. This is what I called `exactP` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(ecdf(exactP), lwd=3, col='blue')\n",
    "plot(ecdf(naiveP), lwd=3, col='red', add=TRUE)\n",
    "abline(0,1, lwd=2, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An exciting area of research\n",
    "\n",
    "This `exactP` was only discovered [recently](http://arxiv.org/abs/1401.3889)!\n",
    "\n",
    "For at least 10 years, I have been saying things along the lines of\n",
    "\n",
    "    Inference after model selection is basically out the window.\n",
    "     Forget all we taught you about t and F distributions as it\n",
    "     is no longer true...\n",
    "    \n",
    "It turns out that inference after selection is possible, and it\n",
    "doesn't force us to throw away all of our tools for inference.\n",
    "\n",
    "*But, it is a little more complicated to describe.*\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Initialization cell\n",
    "%pylab notebook\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first type of model, which we will spend a lot of time on, is the *simple linear regresssion model*. One simple way to think of it\n",
    "is via scatter plots. Below are [heights](http://www.stat.cmu.edu/~roeder/stat707/=data/=data/data/Rlibraries/alr3/html/heights.html) of mothers and daughters collected \n",
    "by Karl Pearson in the late 19th century. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%R -o M,D\n",
    "library(alr3)\n",
    "data(heights)\n",
    "M = heights$Mheight\n",
    "D = heights$Dheight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "heights_fig = plt.figure(figsize=(7,7))\n",
    "axes = heights_fig.gca()\n",
    "axes.scatter(M, D, c='red')\n",
    "axes.set_xlabel(\"Mother's height (inches)\", size=20)\n",
    "axes.set_ylabel(\"Daughter's height (inches)\", size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple linear regression model fits a line through the above scatter plot in a particular way. Specifically, it tries to estimate\n",
    "the height of a new daughter in this population, say $D_{new}$, whose mother had height $H_{new}$. It does this by considering\n",
    "each slice of the data. Here is a slice of the data near $M=66$, the slice is taken over a window of size 1 inch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = 66\n",
    "xf, yf = mlab.poly_between([X-.5,X+.5], [50,50], [75, 75])\n",
    "selected_points = (M <= X+.5) * (M >= X-.5)\n",
    "mean_within_slice = D[selected_points].mean()\n",
    "scatterplot_slice = axes.fill(xf, yf, facecolor='blue', alpha=0.1, hatch='/')[0]\n",
    "axes.set_ylim([50,75])\n",
    "axes.scatter([X],[mean_within_slice], s=130, c='yellow', marker='^')\n",
    "heights_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print mean_within_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see that, in our sample, the average height of daughters whose height fell within our slice is about 65.2 inches. Of course this\n",
    "height varies by slice. For instance, at 60 inches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = 60\n",
    "selected_points = (M <= X+.5) * (M >= X-.5)\n",
    "mean_within_slice = D[selected_points].mean()\n",
    "print mean_within_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression line\n",
    "\n",
    "The regression model puts a line through this scatter plot in an *optimal* fashion.\n",
    "\n",
    "The regression line passes through the point of averages, and its slope is\n",
    "$$\n",
    "\\rho \\cdot \\frac{SD(Y)}{SD(X)}\n",
    "$$\n",
    "where $SD(X)$ and $SD(Y)$ are the sample standard deviations, i.e.\n",
    "$$\n",
    "SD(X) = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X})^2\n",
    "$$\n",
    "$\\rho$ is the sample correlation:\n",
    "$$\n",
    "\\rho = \\frac{\\frac{1}{n} \\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{SD(X) \\cdot SD(Y)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -o slope,intercept\n",
    "parameters = lm(D ~ M)$coef\n",
    "print(parameters)\n",
    "intercept = parameters[1]\n",
    "slope = parameters[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's just make sure that the line passes through the point of\n",
    "averages and has the slope we said it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(mean(D) - slope * mean(M) - intercept)\n",
    "print(cor(D, M) * sd(D) / sd(M))\n",
    "print(c(mean(M) + sd(M), cor(D, M), mean(D) + cor(D, M) * sd(D), mean(D) + sd(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "axes.plot([M.min(), M.max()], [intercept + slope * M.min(), intercept + slope * M.max()], 'k',\n",
    "          linewidth=5)\n",
    "heights_fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SD line\n",
    "\n",
    "The SD line passes through the point of averages and has slope $SD(D)/SD(M)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "SDslope = np.std(D) / np.std(M)\n",
    "SDintercept = np.mean(D) - SDslope * np.mean(M)\n",
    "axes.plot([M.min(), M.max()], [SDintercept + SDslope * M.min(),\n",
    "                               SDintercept + SDslope * M.max()], 'k--',\n",
    "          linewidth=5)\n",
    "heights_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph of averages\n",
    "\n",
    "Let's compare our two lines to the graph of averages, depicted in yellow triangles below.\n",
    "\n",
    "We see that the regression line is quite close to the graph of averages, whereas the SD line is not.\n",
    "\n",
    "The fact that it is the regresssion line that passes closest to the graph of averages instead of the SD line\n",
    "is related to [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def slice_average(X, delta=1):\n",
    "    selected_points = (M <= X+.5*delta) * (M >= X-.5*delta)\n",
    "    mean_within_slice = D[selected_points].mean()\n",
    "    return mean_within_slice\n",
    "\n",
    "axes.scatter(np.arange(55,70), [slice_average(x) for x in np.arange(55, 70)], s=130, c='yellow', marker='^')\n",
    "heights_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Mathematical description of regression line\n",
    "\n",
    "A line $L$ is determined by its slope and intercept:\n",
    "$$\n",
    "L = \\left\\{(y,x): y = \\beta_0 + \\beta_1 x \\right\\}.\n",
    "$$\n",
    "\n",
    "For each line, we can compute the *error sum of squares* on a dataset $D = \\{(X_i,Y_i): 1 \\leq i \\leq n\\}$:\n",
    "$$\n",
    "SSE(L) = SSE(\\beta_0, \\beta_1; X, Y) = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2.\n",
    "$$\n",
    "\n",
    "### Theorem (Gauss)\n",
    "\n",
    "Among all lines, the regression line has the smallest $SSE$.\n",
    "Or,\n",
    "$$\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\hat{\\beta}(X,Y) = (\\hat{\\beta}_0(X,Y), \\hat{\\beta}_1(X,Y)) = \\argmin_{(\\beta_0,\\beta_1)} SSE(\\beta_0, \\beta_1; X, Y).\n",
    "$$\n",
    "\n",
    "Obviously, $\\hat{\\beta}$ is a function of the dataset $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Simple linear regression model\n",
    "\n",
    "* Let $Y_i$ be the height of the $i$-th daughter in the sample, $X_i$ be the height of the $i$-th mother.\n",
    "\n",
    "* Model:\n",
    "   $$Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{\\text{regression equation}} + \\underbrace{\\varepsilon_i}_{\\text{error}}.$$\n",
    "   \n",
    "* Putting a distribution on $\\varepsilon_i$'s and $X_i$'s specifies a *joint distribution* for the $Y$'s and $X$'s. \n",
    "\n",
    "* Alternatively, if we condition on $X$ we only need specify a distribution for $\\varepsilon_i$'s to specify\n",
    "a distribution for $Y_i$'s given $X$.\n",
    "\n",
    "* Most common modelling assumption: the Gaussian model\n",
    "$$\n",
    "\\epsilon_i | X \\overset{IID}{\\sim} N(0, \\sigma^2).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Fitting the model\n",
    "\n",
    "* We will begin by using *least squares* regression. This measures\n",
    "   the *goodness of fit* of a line by the sum of squared errors, $SSE$.\n",
    "   \n",
    "* Least squares regression chooses the line that minimizes\n",
    "   $$\n",
    "   SSE(\\beta_0, \\beta_1) = SSE(\\beta_0, \\beta_1; X, Y) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 \\cdot X_i)^2.$$\n",
    "\n",
    "* In principle, we might measure goodness of fit differently: \n",
    "   $$\n",
    "   SAD(\\beta_0, \\beta_1) = \\sum_{i=1}^n |Y_i - \\beta_0 - \\beta_1 \\cdot X_i|.$$\n",
    "   \n",
    "* For some *loss function* $L$ we might try to minimize\n",
    "    $$\n",
    "    L(\\beta_0,\\beta_1) = \\sum_{i=1}^n L(Y_i-\\beta_0-\\beta_1X_i) \n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Least squares estimators\n",
    "\n",
    "There are explicit formulae for the least squares estimators, i.e. the minimizers of the error sum of squares.\n",
    "\n",
    "For the slope, $\\hat{\\beta}_1$, we will show that\n",
    "$$\n",
    "   \\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y}\n",
    ")}{\\sum_{i=1}^n (X_i-\\overline{X})^2} = \n",
    "\\frac{\\widehat{Cov}(X,Y)}{\\widehat{Var}(X)}.$$\n",
    "\n",
    "Knowing the slope estimate, the intercept estimate can be found easily:\n",
    "$$ \n",
    "\\widehat{\\beta}_0 = \\overline{Y} - \\widehat{\\beta}_1 \\cdot \\overline{X}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Choice of loss function\n",
    "\n",
    "The choice of the function we use to measure goodness of fit, or the *loss* function, \n",
    "determines what\n",
    "sort of estimates we get out of our procedure. For instance, if, instead of fitting a line to a scatterplot, we were\n",
    "estimating a *center* of a distribution, which we denote by $\\mu$, then we might consider minimizing several loss functions.\n",
    "\n",
    "* If we choose the sum of squared errors:\n",
    "$$\n",
    "SSE(\\mu) = \\sum_{i=1}^n (Y_i - \\mu)^2.\n",
    "$$\n",
    "Then, we know that the minimizer of $SSE(\\mu)$ is the sample mean.\n",
    "\n",
    "* On the other hand, if we choose the sum of the absolute errors\n",
    " $$\n",
    "   SAD(\\mu) = \\sum_{i=1}^n |Y_i - \\mu|.$$\n",
    "   Then, the resulting minimizer is the sample median.\n",
    "   \n",
    "* Both of these minimization problems also have *population* versions as well. For instance, the population mean\n",
    "minimizes, as a function of $\\mu$\n",
    "$$\n",
    "\\mathbb{E}((Y-\\mu)^2)\n",
    "$$\n",
    "while the population median minimizes\n",
    "$$\n",
    "\\mathbb{E}(|Y-\\mu|).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Visualizing the loss function\n",
    "\n",
    "Let's take the Hooke's law data from Chapter 2 and visualize its loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([0., 2., 4., 6., 8., 10.])\n",
    "Y = np.array([439.00, 439.12, 439.21, 439.31, 439.40, 439.50])\n",
    "f = plt.figure(figsize=(7,7)); a = f.gca()\n",
    "a.scatter(X, Y, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot the *loss* as a function of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "squared_error_loss = plt.figure(figsize=(7,7))\n",
    "grid = np.mgrid[420:460:100j,(-2):2:100j]\n",
    "squared_errors = (Y[:,None,None] - grid[0] - X[:,None,None] * grid[1])**2\n",
    "axes = squared_error_loss.gca()\n",
    "axes.contour(-squared_errors.sum(0), 20, origin='lower', interpolation='nearest')\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's contrast this with the sum of absolute errors. The contours of absolute\n",
    "loss are piecewise linear, unlike the smooth contours of the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "absolute_error_loss = plt.figure(figsize=(7,7))\n",
    "absolute_errors = np.fabs(Y[:,None,None] - grid[0] - X[:,None,None] * grid[1])\n",
    "axes = absolute_error_loss.gca()\n",
    "axes.contour(-absolute_errors.sum(0), 20, origin='lower', interpolation='nearest')\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting model in `python`\n",
    "\n",
    "We've briefly seen how to fit a model in `R`. Let's take a look at how we might\n",
    "fit it in `python` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "np.random.seed(0)\n",
    "design = sm.add_constant(X)\n",
    "intercept, slope = sm.OLS(Y,design).fit().params\n",
    "intercept, slope, np.sum((Y - intercept - slope*X)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From SSE to geometry\n",
    "\n",
    "Earlier, we assigned each line, determined by  $(\\beta_0, \\beta_1)$ a number, its $SSE$. \n",
    "\n",
    "Given the vectors $X$ and $1$, each points in $\\mathbb{R}^n$, we can form a new point by considering\n",
    "the linear combination\n",
    "$$\n",
    "\\beta_0 \\cdot 1 + \\beta_1 \\cdot X.\n",
    "$$\n",
    "\n",
    "We see that\n",
    "$$\n",
    "SSE(\\beta_0, \\beta_1;X, Y) = \\|Y - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot X)\\|^2\n",
    "$$\n",
    "is the distance between the above point and the vector $Y$.\n",
    "\n",
    "Choosing the line with smallest $SSE$ is therefore the same as choosing the linear combination \n",
    "of $1$ and $X$ that is closest to $Y$. It is the *projection* of $Y$ onto the plane spanned by $\\{1, X\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Geometry of least squares\n",
    "\n",
    "The following picture will be with us, in various guises, throughout much of the course. It depicts\n",
    "the geometric picture involved in least squares regression.\n",
    "\n",
    "<img src=\"http://web.stanford.edu/class/stats203/figs/axes_simple.svg\">\n",
    "\n",
    "It requires some imagination but the picture should be thought as representing vectors in $n$-dimensional space, l where $n$ is the number of points in the scatterplot. In our height data, $n=1375$. The bottom two axes should be thought of as 2-dimensional, while the axis marked \"$\\perp$\" should be thought of as $(n-2)$ dimensional, or, 1373 in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitted values and residuals\n",
    "\n",
    "Having found $\\hat{\\beta}(X,Y)$ we can form the vector of *fitted values*\n",
    "$$\n",
    "\\hat{Y} = \\hat{\\beta}_0 \\cdot 1 + \\hat{\\beta}_1 \\cdot X\n",
    "$$\n",
    "as well as the vector of residuals\n",
    "$$\n",
    "e = Y - \\hat{Y}.\n",
    "$$\n",
    "\n",
    "Note that\n",
    "$$\n",
    "Y = \\hat{Y} + e\n",
    "$$\n",
    "Or, for each data point\n",
    "$$\n",
    "Y_i = \\hat{Y}_i + e_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + e_i.\n",
    "$$\n",
    "\n",
    "Compare this to the regression model\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i.\n",
    "$$\n",
    "To be concrete, let's assume the Gauss model $\\varepsilon_i | X \\overset{IID}{\\sim} N(0,\\sigma^2)$ is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Complexities\n",
    "\n",
    "What is different in the last two equations?\n",
    "\n",
    "- The quantities $\\beta_0, \\beta_1$ are parameters and are not random. They are unknown.\n",
    "\n",
    "- The quantities $\\hat{\\beta}_0, \\hat{\\beta}_1$ are functions of the data. If the data is random, they are therefore\n",
    "random variables.\n",
    "\n",
    "- The data set is observed. We think of these as realizations from our idealized Gauss model.\n",
    "\n",
    "- The residuals $e_i$ are observed: they are functions of the data set. \n",
    "\n",
    "- The errors $\\varepsilon_i$ are unobservable. If we could observe them, then we could find $\\beta_0, \\beta_1$ exactly. (How?)\n",
    "\n",
    "- Because the Gauss model specifies the distribution of $Y_i|X$ (and conditions on $X$) it also can tell\n",
    "us the joint distribution of $(\\hat{\\beta}_0, \\hat{\\beta}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important lengths\n",
    "\n",
    "The (squared) lengths of the above vectors are important quantities in what follows.\n",
    "\n",
    "There are three to note:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   SSE &= \\|Y - \\hat{Y}\\|^2 = \\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
    "   SSR &= \\|\\overline{Y} \\cdot 1 - \\hat{Y}\\|^2 = \\sum_{i=1}^n (\\overline{Y} - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
    "   SST &= \\|Y - \\overline{Y} \\cdot 1\\|^2 = \\sum_{i=1}^n(Y_i - \\overline{Y})^2 = SSE + SSR \\\\\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "(When we introduced $SSE$ it was a function of $(\\beta_0, \\beta_1)$. Above, there are no arguments to $SSE$. Why?)\n",
    "\n",
    "An important summary of the fit is the ratio\n",
    "$$\n",
    "R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n",
    "$$\n",
    "which measures *how much variability in $Y$* is explained by $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Example: wages vs. experience\n",
    "\n",
    "In this example, we'll look at the output of *lm* for the wage\n",
    "data and verify that some of the equations we present for the \n",
    "least squares solutions agree with the output.\n",
    "The data was compiled from a study in econometrics [Learning about Heterogeneity in Returns to Schooling]( http://www.econ.queensu.ca/jae/2004-v19.7/koop-tobias/readme.kt.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "url = 'http://stats191.stanford.edu/data/wage.csv'\n",
    "wages = read.table(url, sep=',', header=T)\n",
    "print(head(wages))\n",
    "mean(logwage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to access the variables in `wages` we `attach` it so that the variables\n",
    "are in the toplevel namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "attach(wages)\n",
    "mean(logwage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "plot(education, logwage, pch=23, bg='red', cex=1.8, cex.lab=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit the linear regression model and add it to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "wages.lm = lm(logwage ~ education)\n",
    "print(wages.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 600 -w 600\n",
    "plot(education, logwage, pch=23, bg='red', cex=1.8, cex.lab=2.5)\n",
    "abline(wages.lm, lwd=4, col='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Gauss model for simple linear regression\n",
    "\n",
    "### Fixed X\n",
    "\n",
    "Recall our Gauss model, where we consider $X$ fixed and model\n",
    "$$\n",
    "Y_i | X \\sim N(\\beta_0 + \\beta_1 \\cdot X_i, \\sigma^2)\n",
    "$$\n",
    "independently for $1 \\leq i \\leq n$.\n",
    "\n",
    "This seems like a good model for the Hooke's law experiment because we can fix the $X$ coordinate to specific values.\n",
    "\n",
    "### Random X\n",
    "\n",
    "In our mother-daughter example, it may not make sense to think of $X$ as fixed. After all,\n",
    "Pearson did not fix the heights of the mothers in the study before collecting the data.\n",
    "\n",
    "The analog of the Gaussian model in the random $X$ setting is to assume that the pairs $(X_i, Y_i)$\n",
    "are jointly Gaussian (more on this soon) and independent.\n",
    "\n",
    "If this Gaussian model is true, scatterplots tend to look like nice data clouds. The heights of mothers\n",
    "and daughters above seem to fit this model.\n",
    "\n",
    "What about `logwage` vs. `education`?\n",
    "\n",
    "What does the regression model mean in this case?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Wages example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "beta.hat.1 = cov(education, logwage) / var(education)\n",
    "beta.hat.0 = mean(logwage) - beta.hat.1 * mean(education)\n",
    "print(c(beta.hat.0, beta.hat.1))\n",
    "print(coef(wages.lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimate of $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is one final quantity needed to estimate all of our parameters in our Gauss model for the scatterplot. This is $\\sigma^2$,\n",
    "the variance of the random variation within each slice (the regression model assumes this variance is constant within each slice...).\n",
    "\n",
    "The estimate most commonly used in the Gauss model is\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i)^2 = \\frac{SSE}{n-2} = MSE\n",
    "$$\n",
    "\n",
    "(Above, note the practice of replacing the quantity $SSE(\\hat{\\beta}_0,\\hat{\\beta}_1)$, i.e. the minimum of this function, with just $SSE$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The term *MSE* above refers to mean squared error: a sum of squares divided by what we call its *degrees of freedom*. The degrees of freedom\n",
    "of *SSE*, the *error sum of squares* is therefore $n-2$. Remember this $n-2$ corresponded to $\\perp$ in the picture above...\n",
    "\n",
    "If our Gauss model is correct, then we will see that\n",
    "$$\n",
    "\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\frac{\\chi^2_{n-2}}{n-2}\n",
    "$$\n",
    "where the right hand side denotes a *chi-squared* distribution with $n-2$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Wages example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "sigma.hat = sqrt(sum(resid(wages.lm)^2) / wages.lm$df.resid)\n",
    "sigma.hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The summary from *R* also contains this estimate of $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "summary(wages.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some theory for simple linear regression model\n",
    "\n",
    "- What can we say about our least squares estimators?\n",
    "\n",
    "- In the Gauss model, we can say a lot. (We will come to this later.)\n",
    "\n",
    "- What if the Gauss model is not correct?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random X model\n",
    "\n",
    "- In principle, our least squares estimators are just functions of a scatter plot, like our \n",
    "`logwage` vs. `education` example.\n",
    "\n",
    "- Perhaps the simplest model is to assume that the points $(X_i, Y_i)_{1 \\leq i \\leq n}$ are independently\n",
    "drawn from some distribution $F$ on $\\mathbb{R}^2$. \n",
    "\n",
    "- In this case, $\\hat{\\beta}=\\hat{\\beta}^n$ is a function of a sample of $n$ independent draws from $F$. What\n",
    "do we expect will happen as $n$ grows?\n",
    "\n",
    "- We saw that\n",
    "$$\n",
    "\\hat{\\beta}^n_1 = \\frac{\\widehat{Cov}^n(X,Y)}{\\widehat{Var}^n(X)},\n",
    "$$\n",
    "where $n$ denotes the size of our sample.\n",
    "\n",
    "- For $n$ large, by the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) (you will \n",
    "certainly see this in some form in STATS 200) we expect\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{Cov}^n(X,Y) &\\approx Cov_F(X,Y) \\\\\n",
    "\\widehat{Var}^n(X) &\\approx Var_F(X). \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "(What random variables are we applying the Law of Large Numbers to?)\n",
    "\n",
    "- Therefore, we expect, for large $n$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}^n_1 & \\approx \\frac{Cov_F(X,Y)}{Var_F(X)} \\overset{def}{=}  \\beta_1(F) \\\\\n",
    "\\hat{\\beta}^n_0 & \\approx E_F(Y) - \\beta_1(F) E_F(X) \\overset{def}{=} \\beta_0(F)\n",
    "\\end{aligned}\n",
    "$$\n",
    "(The notation $\\beta_1(F)$ indicates that if you knew the distribution $F$, you could compute the number to the right of the $\\approx$ symbol.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limiting distribution\n",
    "\n",
    "- In order to construct hypothesis tests about $\\beta_1(F)$ we need to know how close\n",
    "$\\hat{\\beta}^n_1$ is to $\\beta_1(F)$. \n",
    "\n",
    "- These type of results fall under the heading of [Central Limit Theorems](https://en.wikipedia.org/wiki/Central_limit_theorem) (also something you will see in STATS200).\n",
    "\n",
    "- Define $\\epsilon(X,Y)$ by\n",
    "$$\n",
    "Y = \\beta_0(F) + \\beta_1(F)  \\cdot X + \\epsilon(X, Y).\n",
    "$$\n",
    "\n",
    "- We will show that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_F(\\epsilon(X,Y)) &= 0 \\\\\n",
    "E_F(X \\epsilon(X,Y)) &= 0 \\\\\n",
    "\\hat{\\beta}^n_1 &= \\beta_1(F) + \\frac{\\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}) \\epsilon_i}{\\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2} \\\\\n",
    "&= \\beta_1(F) + \\frac{\\overline{X\\cdot \\epsilon} - \\overline{X} \\cdot \\overline{\\epsilon}}{\\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2} \\\\\n",
    "&= \\beta_1(F) + \\frac{\\overline{X\\cdot \\epsilon} - E_F(X) \\cdot \\overline{\\epsilon}}{Var_F(X)} + R_n \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "(where the remainder $R_n$ can be ignored in a precise enough sense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The quantity\n",
    "$$\n",
    "\\frac{\\overline{X\\cdot \\epsilon} - E_F(X) \\cdot \\overline{\\epsilon}}{Var_F(X)}\n",
    "$$\n",
    "is the sample mean of a sequence of centered random variables. The CLT applies to this random variable, it has\n",
    "size roughly $n^{-1/2}$. The remainder has size roughly $n^{-1}$ so is much smaller as $n$ grows.\n",
    "\n",
    "- From this, we will deduce that $\\hat{\\beta}_1^n$ is asymptotically Gaussian *even if our Gauss model does not hold.*\n",
    "\n",
    "- We see that\n",
    "$$\n",
    "Var_F(n^{1/2} (\\hat{\\beta}^n - \\beta_1(F))) = \\frac{Var_F(X \\cdot \\epsilon - E_F(X) \\cdot \\epsilon)}{Var_F(X)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "\n",
    "- The above shows that the least squares estimators\n",
    "while be close to Gaussian for large samples in the random\n",
    "$X$ model.\n",
    "\n",
    "- Usually, an additional assumption is made:\n",
    "$$\n",
    "\\def\\indep{\\perp\\!\\!\\!\\perp}\n",
    "\\epsilon  \\indep X.\n",
    "$$\n",
    "\n",
    "- In this case,\n",
    "$$\n",
    "\\frac{Var_F(X \\cdot \\epsilon - E_F(X) \\cdot \\epsilon)}{Var_F(X)^2} = Var_F(\\epsilon).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theory for the Gauss model, random X\n",
    "\n",
    "- In the Gauss model, where $(X,Y)$ are jointly Normal (i.e. $F$ is a bivariate Normal distribution)\n",
    "we will see that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\epsilon(X,Y) &= Y - E_F[Y|X] \\sim N(0, Var_F(Y|X))\n",
    "\\end{aligned}$$\n",
    "and is independent of $X$. \n",
    "\n",
    "- In the Gauss model, $Var_F(Y|X)$ is a constant that does not depend on $X$. This can be interpreted as the variability\n",
    "within each vertical strip is roughly constant around the regression line.\n",
    "\n",
    "- Further,\n",
    "$$\n",
    "\\frac{Var_F(X \\cdot \\epsilon - E_F(X) \\cdot \\epsilon)}{Var_F(X)^2} = \\frac{Var_F(Y|X)}{Var_F(X)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "variables": {
     "Y": "array([ 439.  ,  439.12,  439.21,  439.31,  439.4 ,  439.5 ])"
    }
   },
   "source": [
    "### Theory for the Gauss model, fixed X\n",
    "\n",
    "* The Gauss model has a very well-developed theory that does not even need\n",
    "asymptotic justification. We will now take a look at this theory.\n",
    "\n",
    "\n",
    "* Let $C$ be the subspace of $\\mathbb{R}^n$ spanned $\\pmb{1}=(1, \\dots, 1)$ and ${X}=(X_1, \\dots, X_n)$.\n",
    "\n",
    "* Then,\n",
    "$${Y} = P_C{Y} + ({Y} - P_C{Y}) = \\widehat{{Y}} + (Y - \\widehat{{Y}}) = \\widehat{{Y}} + e$$\n",
    "\n",
    "* In our model $\\mu=\\beta_0 \\pmb{1} + \\beta_1 {X} \\in C$ so that\n",
    "$$\n",
    "\\widehat{{Y}} = \\mu + P_C{\\varepsilon}, \\qquad {e} = P_{C^{\\perp}}{{Y}} = P_{C^{\\perp}}{\\varepsilon}$$\n",
    " \n",
    "* Our assumption that $\\varepsilon_i$'s are independent $N(0,\\sigma^2)$ tells us that: ${e}$ and $\\widehat{{Y}}$ are independent; $\\widehat{\\sigma}^2 = \\|{e}\\|^2 / (n-2) \\sim \\sigma^2 \\cdot \\chi^2_{n-2} / (n-2)$.\n",
    "\n",
    "- (Why do we know this is true? Follows from some facts about Normal distribution we will soon see.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In turn, this implies\n",
    "$$\n",
    "   \\widehat{\\beta}_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n(X_i-\\overline{X})^2}\\right).$$\n",
    "\n",
    "* Therefore, $$\\frac{\\widehat{\\beta}_1 - \\beta_1}{\\sigma \\sqrt{\\frac{1}{\\sum_{i=1}^n(X_i-\\overline{X})^2}}} \\sim N(\\\n",
    "0,1).$$\n",
    "\n",
    "* The other quantity we need is the *standard error* or SE of $\\hat{\\beta}_1$. This is\n",
    "obtained from estimating the variance of $\\widehat{\\beta}_1$, which, in this case means simply\n",
    "plugging in our estimate of $\\sigma$, yielding\n",
    "$$\n",
    "   SE(\\widehat{\\beta}_1) = \\widehat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n(X_i-\\overline{X})^2}} \\qquad \n",
    "   \\text{independent of $\\widehat{\\beta}_1$}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference for Gauss model, fixed X\n",
    "\n",
    "- This theory for the Gauss model is used for statistical inference. \n",
    "\n",
    "- The most common types of \"statistical inference\" are hypothesis tests and confidence intervals (coming midcourse in STATS200).\n",
    "\n",
    "- Let's look at a preview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Testing $H_0:\\beta_1=\\beta_1^0$\n",
    "\n",
    "* Suppose we want to test that $\\beta_1$ is some pre-specified\n",
    "   value, $\\beta_1^0$ (this is often 0: i.e. is there a linear association)\n",
    "\n",
    "* Under $H_0:\\beta_1=\\beta_1^0$\n",
    "   $$\\frac{\\widehat{\\beta}_1 - \\beta^0_1}{\\widehat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n(X_i-\\overline{X})^2}}}\n",
    "   = \\frac{\\widehat{\\beta}_1 - \\beta^0_1}{ \\frac{\\widehat{\\sigma}}{\\sigma}\\cdot \\sigma \\sqrt{\\frac{1}{\n",
    "\\sum_{i=1}^n(X_i-\\overline{X})^2}}} \\sim t_{n-2}.$$\n",
    "\n",
    "\n",
    "* Reject $H_0:\\beta_1=\\beta_1^0$ if $|T| > t_{n-2, 1-\\alpha/2}$.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Wage example\n",
    "\n",
    "Let's perform this test for the wage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "SE.beta.hat.1 = (sigma.hat * sqrt(1 / sum((education - mean(education))^2)))\n",
    "Tstat = beta.hat.1 / SE.beta.hat.1\n",
    "data.frame(beta.hat.1, SE.beta.hat.1, Tstat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the output of the `lm` function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "summary(wages.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see that *R* performs this test in the second row of the `Coefficients` table. It is clear that\n",
    "wages are correlated with education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why reject for large |T|?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Observing a large $|T|$ is unlikely if $\\beta_1 = \\beta_1^0$: reasonable to conclude that $H_0$ \n",
    "is false.\n",
    "\n",
    "* Common to report $p$-value:\n",
    "$$\\mathbb{P}(|T_{n-2}| > |T|) = 2 \\mathbb{P} (T_{n-2} > |T|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "2*(1 - pt(Tstat, wages.lm$df.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "detach(wages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval based on Student's $t$ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*   Suppose we have a parameter estimate $\\widehat{\\theta} \\overset{\\mathbb{P}_{\\theta}} \\sim N(\\theta, {\\sigma}_{\\theta}^2)$ for each $\\theta$, and standard error $SE(\\widehat{\\theta})$ such that\n",
    "   $$\n",
    "   \\frac{\\widehat{\\theta}-\\theta}{SE(\\widehat{\\theta})} \\overset{\\mathbb{P}_{\\theta}}{\\sim} t_{\\nu}.$$\n",
    "\n",
    "* We can find a $(1-\\alpha) \\cdot 100 \\%$ confidence interval by:\n",
    "   $$\n",
    "   \\widehat{\\theta} \\pm SE(\\widehat{\\theta}) \\cdot t_{\\nu, 1-\\alpha/2}.$$\n",
    "   \n",
    "* To prove this, expand the absolute value in:\n",
    "   $$\n",
    "   1 - \\alpha = \\mathbb{P}_{\\theta}\\left(\\left|\\frac{\\widehat{\\theta} - \\theta}{SE(\\widehat{\\theta})} \\right| < t_{\\nu, 1-\\alpha/2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval for regression parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Applying the above to the parameter $\\beta_1$ yields a confidence interval of the form\n",
    "$$\n",
    "   \\hat{\\beta}_1 \\pm SE(\\hat{\\beta}_1) \\cdot t_{n-2, 1-\\alpha/2}.$$\n",
    "   \n",
    "* We will need to compute $SE(\\hat{\\beta}_1)$. This can be computed using this formula\n",
    "   $$\n",
    "   SE(a_0\\hat{\\beta}_0 + a_1\\hat{\\beta}_1) = \\hat{\\sigma} \\sqrt{\\frac{a_0^2}{n} + \\frac{(a_0\\overline{X} - a_1)^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also need to find the quantity $t_{n-2,1-\\alpha/2}$. This is defined by\n",
    "$$\n",
    "\\mathbb{P}(T_{n-2} \\geq t_{n-2,1-\\alpha/2}) = \\alpha/2.\n",
    "$$\n",
    "In *R*, this is computed by the function `qt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "alpha = 0.05\n",
    "n = length(M)\n",
    "qt(1-0.5*alpha,n-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not surprisingly, this is close to that of the normal distribution, which is a Student's $t$ with $\\infty$ for degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "qnorm(1-0.5*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will not need to use these explicit formulae all the time, as *R* has some built in functions\n",
    "to compute confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "L = beta.hat.1 - qt(0.975, wages.lm$df.resid) * SE.beta.hat.1\n",
    "U = beta.hat.1 + qt(0.975, wages.lm$df.resid) * SE.beta.hat.1\n",
    "data.frame(L, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "confint(wages.lm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

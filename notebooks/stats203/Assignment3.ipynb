{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may discuss homework problems with other students, but you have to prepare the written assignments yourself.\n",
    "\n",
    "Please combine all your answers, the computer code and the figures into one PDF file, and submit a copy to your gradescope account.\n",
    "\n",
    "Grading scheme: 10 points per question, total of 50.\n",
    "Due date: 11:59 PM March 3, 2016 (Thursday evening).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for the problems\n",
    "\n",
    "In the first three questions, we consider a few common \"algorithms\" \n",
    "or \"workflow\" that might be thought of as \"data snooping\". The\n",
    "objective of doing these simulations is to get a sense in which data snooping affects inference.\n",
    "\n",
    "The last two questions are related to penalized regression and comparing the ridge and LASSO estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "In this problem, we will look at a simple form of data snooping and try to make\n",
    "some adjustments. We will be simulating data here, so that we know the answers.\n",
    "\n",
    "1. Write a function, `f1`, that takes arguments $X_{n \\times p}$, a design matrix and $Y_{n \\times 1}$, a response vector\n",
    "and runs one step of forward stepwise regression using AIC as a criterion, taking at most 1 step and returning the resulting `lm` object. What are the possible return values?\n",
    "\n",
    "2. Modify the function so that when a variable has been chosen it returns the $p$-value from the `summary` table, otherwise return `NA`. Call this new function `f2`.\n",
    "\n",
    "3. Call this function many times, with `X=matrix(rnorm(n*p), n, p); Y=rnorm(n)` for $n=100, p=20$, collecting the $p$-values that are not `NA`. Plot the distribution function of the $p$-values using `ecdf`. Do they look like $p$-values from a uniform distribution? What can you conclude about the $p$-values from this algorithm?\n",
    "\n",
    "4. Modify your function again to randomly choose 50% of the data to use with `step`. When at least one variable has been chosen, use that variable and the remaining 50% of the data to compute an appropriate $p$-value. Call this new function `f3`.\n",
    "\n",
    "5. Call this new function many times, with `X=matrix(rnorm(n*p), n, p); Y=rnorm(n)` for $n=100, p=20$, collecting the $p$-values that are not `NA`. Plot the distribution function of the $p$-values using `ecdf`. Do they look like $p$-values from a uniform distribution? What can you conclude about the $p$-values from this algorithm?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "In the setting of Question 1 above, let's consider the function `f2` and the case $p=1$ and $n$ really large\n",
    "so that the relevant $T$ distribution is basically $N(0,1)$.\n",
    "\n",
    "1. Modify the function `f2` so that it returns the $T$ statistic for the model rather than\n",
    "the $p$-value, still returning `NA` when no step was taken. Call the function `f4`.\n",
    "\n",
    "2. In what situations does your function return a $T$ statistic rather than $NA$? Can you\n",
    "express this event in terms of the $T$ statistic (at least approximately)?\n",
    "\n",
    "3. What does part 2. say about the distribution of the $T$ statistics you observe when calling this function? Can you modify `f4` so that it returns a $p$-value that is approximately\n",
    "uniform when `X=rnorm(n); Y=rnorm(n)`? Call this function `f5`.\n",
    "\n",
    "4. Call the function `f5` many times with `X=rnorm(n); Y=rnorm(n)`. Verify that you have found a $p$-value that has the correct behaviour when $X$ is independent of $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 \n",
    "\n",
    "In the midterm, we considered a regression problem something along the lines of\n",
    "$$\n",
    "Y = X\\beta + W\\gamma + Z\\delta + \\epsilon\n",
    "$$\n",
    "where $X_{n \\times k}$, $W_{n \\times 1}$ and $Z_{n \\times 2}$.\n",
    "\n",
    "We had at least two null hypotheses floating around:\n",
    "$$\n",
    "H_{0,Z}: \\delta_1=\\delta_2 = 0\n",
    "$$\n",
    "and\n",
    "$$\n",
    "H_{0,W}: \\gamma = 0.\n",
    "$$\n",
    "\n",
    "For the purposes of this problem, we want specific matrices so take `X=diabetes$x` where `diabetes`\n",
    "can be found in the `lars` package. Generate `W` and `Z` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(lars)\n",
    "data(diabetes)\n",
    "X = diabetes$x\n",
    "Y = diabetes$y\n",
    "set.seed(0)\n",
    "W = rnorm(nrow(X))\n",
    "Z = matrix(rnorm(2*nrow(X)), nrow(X), 2)\n",
    "Z[,1] = Z[,1] + 0.3 * W\n",
    "Z[,2] = Z[,2] - 0.1 * W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When $H_{0,Z}$ is true, there are two different $T$ statistics we can use to test $H_{0,W}$. Compute them and their corresponding $p$-values.\n",
    "\n",
    "2. Having fixed the design `cbind(X,W,Z)`, write a function that takes an argument `Y`,\n",
    "fits a linear regression model with the above design matrix (including an intercept) and \n",
    "returns these two $T$ statistics and corresponding $p$-values. Call this function `g1`.\n",
    "\n",
    "3. Using the coefficients of `lm(Y~X)` and the estimated $\\sigma^2$, write a function that generates $Y$ with these coefficients and noise level under the scenario when $H_{0,W}$ and $H_{0,Z}$ are true.\n",
    "\n",
    "4. Using the data generating model of 3., call `g1` many times and look at the corresponding\n",
    "$p$-values. Do they seem to be uniform?\n",
    "\n",
    "5. Modify the function `g1` so that it first runs the $F$ test to test $H_{0,Z}$. When\n",
    "the decision is not to reject $H_{0,Z}$ at a 5% level, the function should return the two different $p$-values for testing $H_{0,W}$, otherwise return `NA`. Call this function `g2`. Repeat 4. \n",
    "\n",
    "6. There is also an $F$ test to test \n",
    "$$\n",
    "H_{0,W} \\cap H_{0,Z}: \\delta_1=\\delta_2=\\gamma=0.\n",
    "$$\n",
    "Modify function `g2` to return the corresponding $p$-value when the  decision is not to reject $H_{0,Z}$ at a 5% level. Call this function `g3`. Repeat 4.\n",
    "\n",
    "7. Repeat parts 5. and 6. when the decision about $H_{0,Z}$ was to reject it at a 5% level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "In this problem, we will build a regression model using the LASSO and cross-validation.\n",
    "\n",
    "The features can be found [here](http://stats203.stanford.edu/data/X_3TC.csv)\n",
    "and the response [here](http://stats203.stanford.edu/data/Y_3TC.csv). The variables are names\n",
    "[here](http://stats203.stanford.edu/data/muts_3TC.csv).\n",
    "\n",
    "1. Load the response and features into `R` in the variables `X` and `Y`. (Make sure they are matrices by calling: `X=as.matrix(X); Y=as.matrix(Y)`.)\n",
    "\n",
    "2. Choose a model using `cv.glmnet` in the package `glmnet`. This object will have\n",
    "attributes `lambda.min` and `lambda.1se`. The exact coefficients\n",
    "at a specific value of `lambda`, say `l` can be computed as\n",
    "`G=glmnet(X,Y); coef(G, s=l, exact=TRUE)`. Find the model chosen at `lambda.min`. How do the models\n",
    "chosen with these values of `lambda.min` compare to the model chosen in [class](http://nbviewer.jupyter.org/url/web.stanford.edu/class/stats203/notebooks/Data%20snooping.ipynb)? \n",
    "\n",
    "3. Randomly split the data, using 80% of the data with `cv.glmnet` to choose `lambda`. \n",
    "Find the variables chosen by the `lambda.min` rule. \n",
    "Fit a regression model using the\n",
    "remaining 20% of the data and report confidence intervals for the coefficients\n",
    "of all the selected variables. Repeat the procedure many times. Do the same variables\n",
    "always get selected? Even when exactly the same variables are chosen are the $p$-values the same?\n",
    "\n",
    "4. To ensure we all have a consistent set of answers, run `set.seed(1)` and repeat 3. once. Report\n",
    "the variables and confidence intervals for the coefficient of each variable selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Consider the ridge regression algorithm:\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda} = \\text{argmin}_{\\beta} \\frac{1}{2} \\|Y-X\\beta\\|^2_2 + \\frac{\\lambda}{2} \\|\\beta\\|^2_2.\n",
    "$$\n",
    "\n",
    "Suppose \n",
    "$$\n",
    "Y|X=X\\beta^* + \\epsilon, \\qquad \\epsilon \\sim N(0, \\sigma^2 I).\n",
    "$$\n",
    "\n",
    "1. As a function of $\\lambda$, compute\n",
    "$$\n",
    "MSE(\\lambda) = E(\\|\\beta^*-\\hat{\\beta}_{\\lambda}\\|^2_2).\n",
    "$$\n",
    "\n",
    "2. Write a function to generate $Y$ where $\\beta^*$ is the OLS coefficients for the full model in the 3TC data in Question 4, and $\\sigma^2$ is the OLS estimate of $\\sigma^2$ from the full model.\n",
    "Compute\n",
    "$$\n",
    "\\lambda^* = \\text{argmin}_{\\lambda} MSE(\\lambda).\n",
    "$$\n",
    "(This will be easiest to do numerically by plotting $MSE$ as a function of $\\lambda$. You might try to get an expression for $\\frac{d}{d\\lambda}MSE(\\lambda)$ but it will be difficult to solve exactly, though it will be expressible in terms of eigenvalues of $(X^TX)$, $\\lambda$, $\\beta$ and $\\sigma^2$.)\n",
    "\n",
    "3. For these values of\n",
    "$\\beta^*, \\sigma^2$, how much of an improvement in $MSE(\\lambda^*)$ do you see over the OLS estimators (i.e. $\\lambda=0$)? Do your simulations confirm this much of an improvement?\n",
    "\n",
    "4. Use the data generating function in 3. but this time estimate $\\beta$ using the LASSO at `lambda.min` using 5-fold cross-validation on 100% of the data (i.e. do not split the data before finding `lambda.min`). Numerically, how does the $MSE$ of this procedure compare to the best ridge estimator, i.e. the ridge estimator with the optimal value $\\lambda^*$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
